---
title: Binary and Count Prediction
author: Nathaniel Woodward
date: '2019-07-24'
slug: logistic-and-poisson-regression
categories: []
tags: []
---



<p>Let’s enrich our previous discussion of linear regression by considering outcome data that is binary/dichotomous (e.g., success/failure, whether someone recovered or not) and data that is count-based (e.g., number of distinct species per survey site).</p>
<p>In a regular linear regression, we model an outcome <span class="math inline">\(Y\)</span> as a linear function of explanatory variables <span class="math inline">\(X_i\)</span> and error <span class="math inline">\(e\)</span>, like this</p>
<p><span class="math display">\[
Y=\beta_0+\beta_1X_1+\dots+\beta_nX_n+e, \phantom{xxxxx} e\sim N(0,\sigma)
\]</span> Let’s focus on the case of just a single predictor,</p>
<p><span class="math display">\[
Y=\beta_0+\beta_1X_1+e, \phantom{xxxxx} e\sim N(0,\sigma)
\]</span></p>
<p>Recall that the residuals (deviations from data to prediction) are normally distributed, since you can rearrange the above to <span class="math inline">\(Y-\beta_0+\beta_1X_1=e, \phantom{xxxxx} e\sim N(0,\sigma)\)</span></p>
<p>Also, recall that <span class="math inline">\(\beta_0+\beta_1X_1\)</span> is equal to the mean of <span class="math inline">\(Y\)</span> where <span class="math inline">\(X_1=x_i\)</span>. That is <span class="math inline">\(\mu=E[Y|X]=\beta_0+\beta_1X_1\)</span></p>
<p>What happens if we try to run a linear regression on binary response data? Let’s use the <code>biopsy</code> dataset include with the MASS package. This dataset includes nine attributes of tumors and whether they were malignant (1) or benign (0). Let’s predict this outcome from the clump thickness using simple linear regression.</p>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(MASS)</code></pre>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<pre class="r"><code>library(ggplot2)

data&lt;-biopsy%&gt;%transmute(clump_thickness=V1,
                         cell_uniformity=V2,
                         marg_adhesion=V3,
                         bland_chromatin=V7,
                         outcome=class,
                         y=as.numeric(outcome)-1)
head(data)</code></pre>
<pre><code>##   clump_thickness cell_uniformity marg_adhesion bland_chromatin   outcome
## 1               5               1             1               3    benign
## 2               5               4             4               3    benign
## 3               3               1             1               3    benign
## 4               6               8             8               3    benign
## 5               4               1             1               3    benign
## 6               8              10            10               9 malignant
##   y
## 1 0
## 2 0
## 3 0
## 4 0
## 5 0
## 6 1</code></pre>
<pre class="r"><code>fit&lt;-lm(y~clump_thickness,data=data)
summary(fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ clump_thickness, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.77804 -0.17331 -0.01994  0.06859  1.06859 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -0.189535   0.023395  -8.102 2.43e-15 ***
## clump_thickness  0.120947   0.004467  27.078  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3323 on 697 degrees of freedom
## Multiple R-squared:  0.5127, Adjusted R-squared:  0.512 
## F-statistic: 733.2 on 1 and 697 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(data, aes(clump_thickness,y))+geom_jitter(width=.3,height=0)+geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Well, it runs! But as you can see, we are violating many assumptions here (the residuals will certainly not be normal). Also, your outcome data can only be 0/1, but predicted values can be greater than 1 or less than 0, which is not appropriate. For example, someone with a clump thickness of <span class="math inline">\(X=10\)</span> would have a predicted probability of <span class="math inline">\(-.190+.121(10)=1.02\)</span>, or 102%. This is clearly the wrong tool for the job.</p>
<p>Above, we used the structural part of the model (i.e., the part without the error) <span class="math inline">\(\mu=\beta_0+\beta_1X_1\)</span>, where <span class="math inline">\(\mu\)</span> is the mean of <span class="math inline">\(Y\)</span> at a specific value of <span class="math inline">\(X_1\)</span>. This works when the response is continuous and the assumption of normality is met. Instead, however, we could use <span class="math inline">\(g(\mu)=\beta_0+\beta_1X_1\)</span>, where <span class="math inline">\(g()\)</span> is some function that links the response to the predictors in an acceptable, continuous fashion.</p>
<p>In the case of binary outcome data, <span class="math inline">\(g()\)</span> would need to take <span class="math inline">\([0,1]\)</span> data and output continuous data (or vice versa). One idea is to use a cumulative distribution function (CDF). For example, the normal distribution is defined over <span class="math inline">\((-\infty, \infty)\)</span>, but the cumulative probability (the area under the distribution) is defined over <span class="math inline">\([0,1]\)</span>, since <span class="math inline">\(\int_{-\infty}^a \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx \in [0,1]\)</span> for all <span class="math inline">\(a\)</span>. Since the normal CDF <span class="math inline">\(\Phi(z)=P(Z\le z), Z \sim N(0,1)\)</span> maps from all real numbers <span class="math inline">\(z\)</span> to <span class="math inline">\([0,1]\)</span>, it’s inverse <span class="math inline">\(\Phi^{-1}(x)\)</span> will do the opposite.</p>
<pre class="r"><code>#normal CDF
curve(pnorm(x,0,1),-4,4)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>#inverse
curve(qnorm(x,0,1),0,1)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<p>Let’s regress <span class="math inline">\(y\)</span> using the probit link function</p>
<pre class="r"><code>fit&lt;-glm(y~clump_thickness,data=data,family=binomial(link=&quot;probit&quot;))
summary(fit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ clump_thickness, family = binomial(link = &quot;probit&quot;), 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1443  -0.4535  -0.1421   0.1450   3.0332  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -2.83936    0.18083  -15.70   &lt;2e-16 ***
## clump_thickness  0.51487    0.03535   14.56   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 466.66  on 697  degrees of freedom
## AIC: 470.66
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>This equation tells us that for every 1 unit increase in clump thickness (X), the z score increases by .514 on average.</p>
<p><span class="math display">\[
\begin{aligned}
\Phi^{-1}(\widehat{P(\text{malignant} | X)})&amp;=-2.839+0.514X\\
\widehat{P(\text{malignant} | X)}&amp;=\Phi(-2.839+0.514X)\\
\end{aligned}
\]</span></p>
<p>If a patient has a clump thickness of 10, our predicted probability of malignancy is <span class="math inline">\(\Phi(-2.839+0.514*10)=P(Z\le2.301)=.989\)</span></p>
<p>We can get everyone’s predicted z-score (and probability) in this fashion</p>
<pre class="r"><code>data$z_pred&lt;-predict(fit)
data$prob_pred&lt;-predict(fit,type = &quot;response&quot;)

head(data)</code></pre>
<pre><code>##   clump_thickness cell_uniformity marg_adhesion bland_chromatin   outcome
## 1               5               1             1               3    benign
## 2               5               4             4               3    benign
## 3               3               1             1               3    benign
## 4               6               8             8               3    benign
## 5               4               1             1               3    benign
## 6               8              10            10               9 malignant
##   y     z_pred  prob_pred
## 1 0 -0.2650294 0.39549341
## 2 0 -0.2650294 0.39549341
## 3 0 -1.2947609 0.09770136
## 4 0  0.2498364 0.59864305
## 5 0 -0.7798952 0.21772629
## 6 1  1.2795679 0.89965143</code></pre>
<pre class="r"><code>ggplot(data, aes(clump_thickness,y))+geom_jitter(width=.3,height=0)+
  stat_smooth(method=&quot;glm&quot;,method.args=list(family=binomial(link=&quot;probit&quot;)),se=F)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div id="cross-validation-of-probit-model" class="section level2">
<h2>Cross validation of probit model</h2>
<p>Let’s use cross-validation to see how this simple model performs when we train it to part of our full 699 dataset and test how well it predicts the rest. Let’s split the dataset into 10 roughly equal pieces, leave one piece (or “fold”) out (70 observations), fit the model to the remaining 9 folds (629 observations), see how well it classifies the test set by computing false-positive and false-negative rates, and repeat the process so that each fold gets a turn being the one left out (i.e., retrain the model on a new training set, test it on a new test set). Since we get predicted z-scores (or probabilities) from the model, we say the model predicts 1 if <span class="math inline">\(z\ge 0\ (p\ge.5)\)</span>, and 0 otherwise.</p>
<pre class="r"><code>set.seed(1234)

k=10

acc&lt;-fpr&lt;-fnr&lt;-NULL

data1&lt;-data[sample(nrow(data)),]
folds&lt;-cut(seq(1:nrow(data)),breaks=k,labels=F)

for(i in 1:k){
  train&lt;-data1[folds==i,]
  test&lt;-data1[folds!=i,]
  truth&lt;-test$y
  
  fit&lt;-glm(y~clump_thickness,data=train,family=binomial(link=&quot;probit&quot;))
  
  pred&lt;-ifelse(predict(fit,newdata = test)&gt;0,1,0)
  
  acc[i]&lt;-mean(truth==pred)
  fpr[i]&lt;-mean(pred[truth==0])
  fnr[i]&lt;-mean(1-pred[truth==1])
}

mean(acc); mean(fnr); mean(fpr)</code></pre>
<pre><code>## [1] 0.860595</code></pre>
<pre><code>## [1] 0.3299324</code></pre>
<pre><code>## [1] 0.03901361</code></pre>
<p>The probit regression is classifying correctly about 85.49% of the time using just the clump thickness predictor variable. The false negative rate (the percentage of malignancies it identified as benign) was 29.71%; the false positive rate (the percentage of benign tumors it identified as malignant) was 6.45%.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
<p>There is another, more widely used link function for binary data, and that is the logistic function. This approach is based on the odds ratio. Recall that the odds are calculated from a probability like so: <span class="math inline">\(odds(p)=\frac{p}{1-p}\)</span>. The nice thing about this is that we can take a variable <span class="math inline">\(p\)</span> from <span class="math inline">\([0,1]\)</span> and transform it. However, as <span class="math inline">\(p\)</span> approaches 0, the odds approach 0, and as <span class="math inline">\(p\)</span> approaches 1, the odds approach <span class="math inline">\(\infty\)</span>.</p>
<pre class="r"><code>curve(x/(1-x),from = 0, 1)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>If we take the logarithm of the odds (use the natural log for convenience), you now have variable taking values from <span class="math inline">\([-\infty,\infty]\)</span>. This function, the logarithm of the odds, is called the <em>logit</em> function: it is where logistic regression gets its name. Below is a graph of <span class="math inline">\(y=log(\frac{p}{1-p})\)</span> shown alongside the dotted normal quantile function (i.e., <span class="math inline">\(\Phi^{-1}(x)\)</span></p>
<pre class="r"><code>curve(log(x/(1-x)),0,1)
curve(qnorm(x,0,1),0,1,add=T,lty=2)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Very similar! Using this link function to convert our 0, 1 values to a continuous scale, we write the logistic regression equation</p>
<p><span class="math display">\[
log(\frac{p}{1-p})=\beta_0+\beta_1X
\]</span> Where <span class="math inline">\(p\)</span> is the <span class="math inline">\(p(Y=1 \mid X=x)\)</span>.</p>
<p>Let’s try using this link function in our regression of malignancy on clump thickness.</p>
<pre class="r"><code>fit&lt;-glm(y~clump_thickness,data=data,family=binomial(link=&quot;logit&quot;))
summary(fit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ clump_thickness, family = binomial(link = &quot;logit&quot;), 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1986  -0.4261  -0.1704   0.1730   2.9118  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -5.16017    0.37795  -13.65   &lt;2e-16 ***
## clump_thickness  0.93546    0.07377   12.68   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 464.05  on 697  degrees of freedom
## AIC: 468.05
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<pre class="r"><code>data$logit_pred&lt;-predict(fit)</code></pre>
<p>What does this tell us? Well first, clump thickness appears to be strongly and positively predictive of malignancy. The equation with the parameter estimates is <span class="math inline">\(log(\frac{p}{1-p})=-5.160+0.935X\)</span>. You can see that for every one-unit increase in X (clump thickness), the log-odds goes up by .935, which means that the odds change by a factor of <span class="math inline">\(e^{.935}=2.55\)</span> (i.e., they go up by 155%)! If you go up two clump-thickesses, we get a change in odds of <span class="math inline">\(e^{2\times.935}=e^{.935}e^{.935}=2.55\times 2.55\approx6.50\)</span>, which is <span class="math inline">\(2.55\times 2.55\)</span>.</p>
<p>How about getting a predicted probability for a person with a clump thickness of <span class="math inline">\(X\)</span>? For a person with <span class="math inline">\(X=10\)</span>, for example, we get a logit score of <span class="math inline">\(4.19\)</span> (by plugging into the equation above). But how do we invert the logit function to get the predicted probability out? Just use algebra: first exponentiate to remove the log, then multiply both sides by the denominator, then distribute, the get all terms with <em>p</em> on the same side, then factor out the <em>p</em>, then solve for it!</p>
<p><span class="math display">\[
\begin{aligned}
log(\frac{p}{1-p})&amp;=\beta_0+\beta_1X\\
\frac{p}{1-p}&amp;=e^{\beta_0+\beta_1X}\\
p&amp;=(1-p)e^{\beta_0+\beta_1X}\\
p&amp;=e^{\beta_0+\beta_1X}-pe^{\beta_0+\beta_1X}\\
p+pe^{\beta_0+\beta_1X}&amp;=e^{\beta_0+\beta_1X}\\
p(1+e^{\beta_0+\beta_1X})&amp;=e^{\beta_0+\beta_1X}\\
p&amp;=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}\\
\end{aligned}
\]</span></p>
<p>In general, <span class="math inline">\(p=\frac{odds}{1+odds}\)</span>. Note that if you multiply top and bottom of the RHS by <span class="math inline">\(e^{-\beta_0+\beta_1X}\)</span> (note the negative), you get the alternative formulation <span class="math inline">\(p=\frac{1}{1+e^{-\beta_0+\beta_1X}}\)</span>.</p>
<p>So we take what the regression evaluates to, raise <span class="math inline">\(e\approx2.71828\)</span> to that power, and divide the result by <span class="math inline">\(1 +\)</span> itself.</p>
<pre class="r"><code>exp(4.19)/(1+exp(4.19))</code></pre>
<pre><code>## [1] 0.9850797</code></pre>
<p>A 98.5% chance! A bit smaller than what the probit model predicted. We can also get this directly from R. Let’s ask R for the probability of malignancy at each level of clump thickness (i.e., from 1 to 10).</p>
<pre class="r"><code>predict(fit,newdata=data.frame(clump_thickness=1:10),type=&quot;response&quot;)</code></pre>
<pre><code>##          1          2          3          4          5          6 
## 0.01441866 0.03594186 0.08676502 0.19492345 0.38157438 0.61125443 
##          7          8          9         10 
## 0.80028036 0.91080524 0.96299397 0.98514461</code></pre>
<pre class="r"><code>data$logit&lt;-predict(fit)

ggplot(data, aes(clump_thickness,y))+geom_jitter(width=.3,height=0)+stat_smooth(method=&quot;glm&quot;,method.args=list(family=&quot;binomial&quot;),se=F)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>data$outcome&lt;-factor(data$outcome,levels=c(&quot;malignant&quot;,&quot;benign&quot;))
ggplot(data,aes(logit, fill=outcome))+geom_density(alpha=.3)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>Let’s see how it fares under cross-validation. We will start by using the exact same procedure as we did above.</p>
<pre class="r"><code>acc&lt;-fpr&lt;-fnr&lt;-NULL

for(i in 1:k){
  train&lt;-data1[folds==i,]
  test&lt;-data1[folds!=i,]
  truth&lt;-test$y
  
  fit&lt;-glm(y~clump_thickness,data=train,family=binomial(link=&quot;logit&quot;))
  
  pred&lt;-ifelse(predict(fit,newdata = test)&gt;0,1,0)
  
  acc[i]&lt;-mean(truth==pred)
  fpr[i]&lt;-mean(pred[truth==0])
  fnr[i]&lt;-mean(1-pred[truth==1])
}

mean(acc); mean(fnr); mean(fpr)</code></pre>
<pre><code>## [1] 0.860595</code></pre>
<pre><code>## [1] 0.3299324</code></pre>
<pre><code>## [1] 0.03901361</code></pre>
<p>The logistic regression accurately predicted out of sample exactly as accurately as the probit model; the false negative and false positive rates were the same as well!</p>
<p>I’m curious what we will see using another supervised classification approach: linear discriminant analysis (LDA). I won’t go into details here, but just for the sake of comparison let’s run it.</p>
<pre class="r"><code>acc&lt;-fpr&lt;-fnr&lt;-NULL

for(i in 1:k){
  train&lt;-data1[folds==i,]
  test&lt;-data1[folds!=i,]
  truth&lt;-test$y
  
  fit&lt;-lda(y~clump_thickness,data=train)
 
  pred&lt;-predict(fit,test)
  tab&lt;-table(pred$class,test$outcome)
  
  acc[i]=sum(diag(tab))/sum(tab)
  fpr[i]&lt;-tab[2,1]/sum(tab[,1])
  fnr[i]&lt;-tab[1,2]/sum(tab[,2])
}

mean(acc); mean(fnr); mean(fpr)</code></pre>
<pre><code>## [1] 0.860595</code></pre>
<pre><code>## [1] 0.3299324</code></pre>
<pre><code>## [1] 0.03901361</code></pre>
<p>Hmm, ever so slightly worse in the accuracy and false negative department, but slightly lower false positive rate. Still, very close!</p>
<div id="leave-one-out-cv" class="section level4">
<h4>Leave-One-Out CV</h4>
<p>Let’s take this opportunity to introduce another kind of cross-validation technique: Leave-one-out cross-validation (or LOOCV).</p>
<p>Just like it sounds we train the model on <span class="math inline">\(n-1\)</span> datapoints and use it to predict the omitted point. We do this for every observation in the dataset (i.e., each point gets a turn being left out and we try to predicted it from all of the rest). Let’s try it on the logistic regression.</p>
<pre class="r"><code>acc&lt;-fpr&lt;-fnr&lt;-NULL

for(i in 1:length(data)){

  train&lt;-data[-i,]
  test&lt;-data[i,]
  truth&lt;-test$y
  
  fit&lt;-glm(y~clump_thickness,data=train,family=binomial(link=&quot;logit&quot;))
  
  pred&lt;-ifelse(predict(fit,newdata = test)&gt;0,1,0)
  
  acc[i]&lt;-truth==pred
  if(truth==0 &amp; pred==1) fpr[i]&lt;-1
  if(truth==1 &amp; pred==0) fnr[i]&lt;-1
}

mean(acc)</code></pre>
<pre><code>## [1] 0.9</code></pre>
</div>
<div id="categorical-predictor-variables" class="section level3">
<h3>Categorical predictor variables</h3>
<p>Imagine that instead, clump size was a categorical variable with three levels, say <em>small</em>, <em>medium</em>, and <em>large</em>. I will artificially create this variable using terciles because the dataset doesn’t actually contain any categorical variables for us to play with (and the interpretation of the coefficients is slightly different).</p>
<pre class="r"><code>data$clump_cat&lt;-cut(data$clump_thickness,breaks=quantile(data$clump_thickness,0:3/3),include.lowest = T)
data$clump_cat&lt;-factor(data$clump_cat,labels=c(&quot;S&quot;,&quot;M&quot;,&quot;L&quot;))

fit1&lt;-glm(y~clump_cat, data=data, family=binomial(link=&quot;logit&quot;))
summary(fit1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ clump_cat, family = binomial(link = &quot;logit&quot;), 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0886  -0.3599  -0.3599   0.4895   2.3534  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -2.7045     0.2370 -11.413  &lt; 2e-16 ***
## clump_catM    1.7171     0.2833   6.062 1.34e-09 ***
## clump_catL    4.7660     0.3314  14.381  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 518.73  on 696  degrees of freedom
## AIC: 524.73
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>First of all, notice that the residual deviance and the AIC are <em>higher</em> now than the were before we artificially discretized this variable. This serves to illustrate the value of keeping your data numeric rather than using it to create groups: here, we are wasting good information and getting poorer model fit.</p>
<p>Notice that there are two effect estimates in the output: one for the medium category (M) and one for the large category (L). By default, regressions are dummy coded such that the estimates are compared to the reference category (the one not listed; in this case, the small category). Here, the coefficients are log odds ratios; if we exponentiate, we get odds ratios compared to the reference category. For example, <span class="math inline">\(e^{1.717}=5.57\)</span>, so the odds of malignancy are 5.57 times as high for those with medium clump sizes as those with small clump sizes, and this difference is significant. This difference is of course more pronounced for the large clump sizes: compared to those with small clump sizes, the odds of malignancy among those with large clump sizes are <span class="math inline">\(e^{4.766}=117.45\)</span> times as high!</p>
<p>To see that this is true, let’s calculate the odds ratios by hand.</p>
<pre class="r"><code>table(data$y,data$clump_cat)</code></pre>
<pre><code>##    
##       S   M   L
##   0 284 153  21
##   1  19  57 165</code></pre>
<p>The odds of cancer (y=1) if you have small clump thickness is <span class="math inline">\(19/284=.0669\)</span> and the odds for medium and large are <span class="math inline">\(57/153=.3725\)</span> and <span class="math inline">\(7.8571\)</span>. The odds ratio of M-to-S is <span class="math inline">\(\frac{.3725}{.0669}=5.57\)</span> and L-to-S is <span class="math inline">\(\frac{7.8571}{.0669}=117.45\)</span></p>
<p>We can even use these numbers to calculate the standard errors. The estimated standard error of a <span class="math inline">\(log\ OR\)</span> is just <span class="math inline">\(\sqrt{\frac 1a + \frac 1b +\frac 1c + \frac 1d}\)</span> where <span class="math inline">\(a, b, c,\)</span> and <span class="math inline">\(d\)</span> are the counts with/without cancer in each of the two conditions being compared. For example, comparing M-to-S, <span class="math inline">\(SE=\sqrt{\frac{1}{284}+\frac{1}{19}+\frac{1}{153}+\frac{1}{57}}=.2833\)</span>, which matches the output above. You can use this to perform hypothesis tests using the normal distribution.</p>
</div>
<div id="multiple-predictor-variables" class="section level3">
<h3>Multiple predictor variables</h3>
<p>Let’s add more variables to our model to try to improve prediction. Let’s try adding bland chromatin and marginal adhesion. Our model becomes</p>
<pre class="r"><code>fit2&lt;-glm(y~clump_thickness+marg_adhesion+bland_chromatin, data=data, family=&quot;binomial&quot;)
summary(fit2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ clump_thickness + marg_adhesion + bland_chromatin, 
##     family = &quot;binomial&quot;, data = data)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -3.11645  -0.16339  -0.08323   0.03270   2.94043  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -9.0911     0.8262 -11.003  &lt; 2e-16 ***
## clump_thickness   0.6137     0.1069   5.744 9.26e-09 ***
## marg_adhesion     0.8457     0.1257   6.729 1.71e-11 ***
## bland_chromatin   0.7404     0.1267   5.845 5.07e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 165.23  on 695  degrees of freedom
## AIC: 173.23
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>Notice that you can be lazy with your <code>family=</code> specification for logistic regression. Wow, so all of these variables are significant. Notice that the AIC is 173.23, compared to our previous model’s 468.05! Lower AIC/deviance is better. We could formally test the additional explanatory value of the new variables by performing an analysis of deviance with the null hypothesis that the more basic model is the true model.</p>
<p>The way this works under the hood is, we compute the likelihood of each model (the full/saturated models) and compare it to the likelihood of the basic (null) model (with no predictors). We take the ratio of these likelihoods (full to null), take the log of these ratios, and multiply them each by <span class="math inline">\(-2\)</span>, resulting in something called a deviance. For example, the deviance of the first model is 464.05 (from the output below), while the deviance for the model with three predictors is 165.23 (smaller: a good thing!). Subtracting this statistic follows a chi-squared distribution with DF equal to the number of additional parameters in the full model. That is, we compute</p>
<pre class="r"><code>fit&lt;-glm(y~clump_thickness,data=data,family=binomial(link=&quot;logit&quot;))
anova(fit,fit2,test=&quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: y ~ clump_thickness
## Model 2: y ~ clump_thickness + marg_adhesion + bland_chromatin
##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
## 1       697     464.05                          
## 2       695     165.23  2   298.82 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Yes, it appears that the full model is better! We can get a goodness-of-fit estimate analogous to <span class="math inline">\(R^2\)</span> by seeing what proportion of the total deviance in the null model can be accounted for by the full model. You just put the difference in deviances in the numerator and the null deviance in the denominator, like this.</p>
<pre class="r"><code>(fit2$null.deviance-fit2$deviance)/fit2$null.deviance</code></pre>
<pre><code>## [1] 0.8165162</code></pre>
<p>Wow, quite high! This quantity is known as the Hosmer and Lemeshow pseudo <span class="math inline">\(R^2\)</span>; it is analogous to traditional <span class="math inline">\(R^2\)</span>, but there are alternatives (e.g., Nagelkerke).</p>
<p>Here is the fitted model with the parameter estimates:</p>
<p><span class="math display">\[
log(\frac{p}{1-p})=-9.09+0.61Clump+0.85MargAd+0.74BlandChrom
\]</span></p>
<p>Holding marginal adhesion and bland chromatin constant, we see an <span class="math inline">\(e^{.61}=1.84\)</span> change in the odds of developing breast cancer for every one-unit increase in clump thickness. Specifically, we see an 84% increase in the odds of cancer for each increase in clump thickness.</p>
<p>How much better does this perform? Let’s rerun our cross-validation. First, 10-fold:</p>
<pre class="r"><code>acc&lt;-fpr&lt;-fnr&lt;-NULL

for(i in 1:k){
  train&lt;-data[folds==i,]
  test&lt;-data[folds!=i,]
  truth&lt;-test$y
  
  fit&lt;-glm(y~clump_thickness+marg_adhesion+bland_chromatin,data=train,family=binomial(link=&quot;logit&quot;))
  
  pred&lt;-ifelse(predict(fit,newdata = test)&gt;0,1,0)
  
  acc[i]&lt;-mean(truth==pred)
  fpr[i]&lt;-mean(pred[truth==0])
  fnr[i]&lt;-mean(1-pred[truth==1])
}</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre class="r"><code>mean(acc); mean(fnr); mean(fpr)</code></pre>
<pre><code>## [1] 0.9434098</code></pre>
<pre><code>## [1] 0.09410461</code></pre>
<pre><code>## [1] 0.03666142</code></pre>
<p>Wow, very good prediction! We are accurately predicting out-of-sample 94.3% of the time, with a 9.4% false negative rate and a 3.7% false positive rate.</p>
</div>
</div>
<div id="sensitivity-specificity-and-roc-curves" class="section level2">
<h2>Sensitivity, Specificity, and ROC Curves</h2>
<p>We could try to visualize this logistic regression, but we would need an axis for each additional predictor variable. We could just plot one predictor at a time on the x-axis. Instead, let’s use PCA to find the best linear combination of our three predictors, resulting in a single variable that summarizes as much of the variability in the set of three as possible. We can this use this as a predictor (note this is really just for visualization purposes; just pretend we ran the logistic regression with a single variable called <code>predictor</code>).</p>
<pre class="r"><code>pca1&lt;-princomp(data[c(&#39;clump_thickness&#39;,&#39;marg_adhesion&#39;,&#39;bland_chromatin&#39;)])
data$predictor&lt;-pca1$scores[,1]

fit&lt;-glm(y~predictor,data=data,family=&quot;binomial&quot;)
data$prob_pred&lt;-predict(fit,type=&quot;response&quot;)

ggplot(data, aes(predictor,prob_pred))+geom_point(aes(color=outcome),alpha=.5,size=3)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>If we give anyone above <span class="math inline">\(p(y=1|x)=.5\)</span> a prediction of malignancy, and anyone below a prediction of benign, we can calculate the true positive rate (Sensitivity), the true negative rate (Specificity), and the corresponding false positive/negative rates.</p>
<pre class="r"><code>ggplot(data, aes(predictor,prob_pred))+geom_point(aes(color=outcome),alpha=.5,size=3)+geom_rug(aes(color=outcome),sides=&quot;right&quot;)+geom_hline(yintercept=.5)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Any blue dots above the line are false positives: any reds below the line are false negatives. For example, the Sensitivity (or true positive rate, TPR) is the number of red dots above the line (which are predicted positive, since <span class="math inline">\(p(y=1)&gt;.5\)</span>) out of the total number of red dots.</p>
<pre class="r"><code>sens&lt;-sum(data[data$y==1,]$prob_pred&gt;=.5)/sum(data$y)
sens</code></pre>
<pre><code>## [1] 0.9419087</code></pre>
<p>Think of it as the probability of detecting the disease, given that it exists (i.e., probability of detection). It is the percentage of <em>actual positives</em> (e.g., people with the disease) that are correctly flagged as positive. Thus, Sensitivity is the degree to which actual positives are not overlooked: A test that is highly sensitive (e.g., <span class="math inline">\(&gt;.90\)</span>) rarely overlooks a thing it is looking for. The higher the Sensitivity, the lower the false negative rate (type II error).</p>
<p>Compare this to the Specificity (true negative rate, TNR), the proportion of actual negatives (e.g., healthy people) that are correctly flagged as negative. It is the probability of detecting the absence of the disease, given that it does not exist! Thus, it indexes the avoidance of false positives: the higher the Specificity, the lower the false positive rate (1-Specificity, or type I error). A test that is highly specific rarely mistakes something else for the thing it is looking for.</p>
<p>Here, Specificity is the percentage of blue dots that fall below the horizontal cut-off (i.e., of those with <span class="math inline">\(p(y=1)&lt;.5\)</span>, the percentage that are actually <span class="math inline">\(y=0\)</span>).</p>
<pre class="r"><code>spec&lt;-sum(data[data$y==0,]$prob_pred&lt;.5)/sum(data$y==0)
spec</code></pre>
<pre><code>## [1] 0.9694323</code></pre>
<p>You can put confidence intervals on these estimates (e.g., using large-sample approximations for binomial)</p>
<pre class="r"><code>#sensitivity
sens+c(-1.96,1.96)*sqrt(sens*(1-sens)/sum(data$y))</code></pre>
<pre><code>## [1] 0.9123757 0.9714417</code></pre>
<pre class="r"><code>#specificity
spec+c(-1.96,1.96)*sqrt(spec*(1-spec)/sum(data$y==0))</code></pre>
<pre><code>## [1] 0.9536666 0.9851980</code></pre>
<p>This quickly gets confusing, so it is a good idea to make a table like this (indeed, it is referred to as a confusion matrix):</p>
<pre class="r"><code>data$test&lt;-as.factor(ifelse(predict(fit)&gt;0,&quot;positive&quot;,&quot;negative&quot;))
data$disease&lt;-data$outcome

with(data, table(test,disease))%&gt;%addmargins</code></pre>
<pre><code>##           disease
## test       malignant benign Sum
##   negative        14    444 458
##   positive       227     14 241
##   Sum            241    458 699</code></pre>
<p>As before, Sensitivity is the proportion of those with the desease who test positive, <span class="math inline">\(p(+|malignant)=\frac{227}{241}=.942\)</span>, while Specificity is the proportion of those who do <em>not</em> have the disease who test negative, <span class="math inline">\(p(-|benign)=\frac{444}{458}=.969\)</span>.</p>
<p>Here is a helpful plot of the proportions testing positive/negative in each disease category, to help keep things straight visually:</p>
<pre class="r"><code>library(magrittr)

plotdat&lt;-data.frame(rbind(
  cbind(density(data$logit_pred[data$outcome==&quot;malignant&quot;])%$%data.frame(x=x,y=y),disease=&quot;malignant&quot;),
  cbind(density(data$logit_pred[data$outcome==&quot;benign&quot;])%$%data.frame(x=x,y=y),disease=&quot;benign&quot;)))%&gt;%mutate(test=ifelse(x&gt;0,&#39;positive&#39;,&#39;negative&#39;),`disease.test`=interaction(disease,test))

plotdat%&gt;%ggplot(aes(x,y,fill=`disease.test`))+geom_area(alpha=.6,color=1)+geom_vline(xintercept=0,size=1)+
    theme(axis.title.y=element_blank(),legend.position=c(.87,.8))+xlab(&quot;Predicted logit&quot;)+
    geom_text(x=-3,y=.1,label=&quot;TN&quot;)+
    geom_text(x=-1,y=.015,label=&quot;FN&quot;)+
    geom_text(x=.5,y=.015,label=&quot;FP&quot;)+
    geom_text(x=3,y=.05,label=&quot;TP&quot;)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>This shows the density of benign (hump on left side: green + purple) and the density of malignant (right side: blue + red): both humps add up to 100%.</p>
<ul>
<li>green represents the proportion of benign that got a negative test result (True Negative rate: Specificity)</li>
<li>purple represents the proportion of benign that got a positive test result (False Positive rate)</li>
<li>blue represents the proportion of malignant that got a positive test results (True Positive rate: Sensitivity)</li>
<li>red represents the proportion of malignant that got a negative test result (False Negative rate)</li>
</ul>
<p>Just like in traditional null hypothesis testing, there is a trade-off between type I and type II error (and so there is a trade-off between sensitivity and specificity). The stricter your test, the fewer false positives it will make (and thus the higher the Specificity), but the more false negatives. The laxer your test, the fewer false negatives it will make (and thus the higher the Sensitivity or Power), but it will allow in more false positives.</p>
<p>Notice that a lot depends on the cut-off! For example, if we set the cut-off at 10% (thus making it a laxer criterion), so that a predicted probability of 10% or more means we predict malignant (i.e., 1), otherwise benign (0), we get</p>
<pre class="r"><code>ggplot(data, aes(predictor,prob_pred))+geom_point(aes(color=outcome),alpha=.5,size=3)+geom_rug(aes(color=outcome),sides=&quot;right&quot;)+geom_hline(yintercept=.1)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre class="r"><code>#sensitivity
sum(data[data$y==1,]$prob_pred&gt;.1)/sum(data$y)</code></pre>
<pre><code>## [1] 0.9875519</code></pre>
<pre class="r"><code>#specificity
sum(data[data$y==0,]$prob_pred&lt;.1)/sum(data$y==0)</code></pre>
<pre><code>## [1] 0.8973799</code></pre>
<p>By lowering our cut-off we have improved sensitivity (and lowered the false negative rate: very few red dots are below the line), but we have worsened our specificity (we have increased the false positive rate: we are more likely to say a patient doesn’t have cancer when they do; there is a greater proportion of blue dots above the line). This tradeoff always occurs:</p>
<pre class="r"><code>true_pos&lt;-function(x,data=data) sum(data[data$y==1,]$prob_pred&gt;x)/sum(data$y)
false_pos&lt;-function(x,data=data) sum(data[data$y==0,]$prob_pred&gt;x)/sum(data$y==0)

TPR&lt;-sapply(seq(0,1,.01),true_pos,data)
FPR&lt;-sapply(seq(0,1,.01),false_pos,data)

ROC1&lt;-data.frame(TPR,FPR,cutoff=seq(0,1,.01))

library(tidyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;tidyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:magrittr&#39;:
## 
##     extract</code></pre>
<pre class="r"><code>ROC1%&gt;%gather(key,value,-cutoff)%&gt;%ggplot(aes(cutoff,value,color=key))+geom_path()</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>What happens if we plot the true positive rate (Sensitivity) versus the false positive rate (1-Specificity) for all possible cut-offs?</p>
<pre class="r"><code>ROC1%&gt;%ggplot(aes(FPR,TPR))+geom_path(size=1.5)+geom_segment(aes(x=0,y=0,xend=1,yend=1),lty=2)+scale_x_continuous(limits = c(0,1))</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>This is an ROC (Receiver Operating Characteristic) curve. If we were predicting perfectly with no mistakes, the TPR wouldbe 1 while the FPR would be 0 regardless of the cutoff and thus the curve would extend all the way to the upper right corner (100% sensitivity and 100% specificity). The dotted line shows the worst-case scenario (the FPR equal to the TPR, so our test is just as good as randomly assigning people positive and negative results), while the thick curve is our emprical ROC (pretty good!).</p>
<p>You can use this plot to find the optimal cutoff. Usually, this is defined as <span class="math inline">\(max(TPR-FPR)\)</span> but there are lots of alternatives. We can find out what this is quickly</p>
<pre class="r"><code>ROC1[which.max(ROC1$TPR-ROC1$FPR),]</code></pre>
<pre><code>##          TPR        FPR cutoff
## 18 0.9792531 0.05021834   0.17</code></pre>
<div id="auc" class="section level3">
<h3>AUC</h3>
<p>The area under the curve (AUC) quantifies how well we are predicting overally and it is easy to calculate: just connect every two adjacent points with a line and then drop a vertical line from each point to the x-axis to form a trapezoid. Then just calculate the area of each trapezoid, <span class="math inline">\((x_1-x_0)(y_0+y_1)/2\)</span> and add them all together.</p>
<pre class="r"><code>#order dataset from least to greatest
ROC1&lt;-ROC1[order(-ROC1$cutoff),]

widths&lt;-diff(ROC1$FPR)
heights&lt;-vector()

for(i in 1:100) heights[i]&lt;-ROC1$TPR[i]+ROC1$TPR[i+1]

AUC&lt;-sum(heights*widths/2)
AUC%&gt;%round(4)</code></pre>
<pre><code>## [1] 0.9903</code></pre>
<p>You can interpret the AUC as the probability that a randomly selected person <em>with cancer</em> has a higher predicted probability of having cancer than a randomly selected person <em>without cancer</em>.</p>
<p>By way of an important, interesting aside, you might be thinking that this sounds like the usual interpretation for a Wilcoxon test statistic W (the non-parametric alternative to Student’s t test, sometimes called Mann-Whitney U). To refresh, this is a nonparametric statistic used to test whether the level of some numeric variable in one population tends to be greater/less than in another population without making any assumptions about how the variable is distributed, with the null hypothesis that the numeric variable is a useful discriminator: that an value of the numeric variable is just as likely to be from an individual in the first population as from an individual in the second. To calculate this statistic, you compare every observation in your first sample to every observation in your second sample, recording 1 if the first is bigger, 0 if the second is bigger, and 0.5 if they are equal. Then you average these across all possible combinations to get your U statistic.</p>
<p>Let’s calculate this statistic for a binary prediction situation: We will compute how well the scores (i.e., the predicted probabilities of cancer) discriminate between those with cancer (sample 1) and those without (sample 2). Our malignant sample consists of 241 individuals and our benign sample consists of 458 individuals. Comparing each individual’s predicted outcome in one sample to each individual’s predicted outcome in the other means we make <span class="math inline">\(241\times 458=110378\)</span> comparisons. Using the base R function <code>expand.grid()</code> we generate a row for every possible comparison. Then we check if predicted probability of disease is higher for malignant than for benign for every row and sum up the yesses.</p>
<pre class="r"><code>pos&lt;-data[which(data$disease==&quot;malignant&quot;),]$prob_pred
neg&lt;-data[which(data$disease==&quot;benign&quot;),]$prob_pred

n1&lt;-length(pos)
n2&lt;-length(neg)

expand.grid(pos=pos,neg=neg)%&gt;%summarize(W=sum(ifelse(pos&gt;neg,1,ifelse(pos==neg,.5,0))),n=n(),W/n)</code></pre>
<pre><code>##          W      n       W/n
## 1 109309.5 110378 0.9903196</code></pre>
<pre class="r"><code>wilcox.test(pos,neg)</code></pre>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  pos and neg
## W = 109310, p-value &lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<pre class="r"><code>wilcox.test(pos,neg)$stat/(n1*n2)</code></pre>
<pre><code>##         W 
## 0.9903196</code></pre>
<p>The W (or U) statistic itself counts the number of times that the predicted probability for malignant individuals is higher than for benign individuals. Here we can see that the proportion of times that malignant individuals have a higher predicted probability than beningn individuals is .9903 (for all possible comparisons). This quantity is the same as the AUC!</p>
<p>The nice thing is, we can calculate a standard error for the AUC using the large-sample normal aproximation</p>
<pre class="r"><code>#see Fogarty, Baker, and Hudson 2005 for details
Dp&lt;-(n1-1)*((AUC/(2-AUC))-AUC^2)
Dn&lt;-(n2-1)*((2*AUC^2)/(1+AUC)-AUC^2)

sqrt((AUC*(1-AUC)+Dp+Dn)/(n1*n2))</code></pre>
<pre><code>## [1] 0.00448658</code></pre>
<p>Several packages have out-of-the-box ROC curve plotters and AUC calculators. For example, here’s the package <code>plotROC</code>’s <code>geom_roc()</code> function that plays nicely with ggplot2, and it’s <code>calc_auc()</code> function, which you can use on a plot object that uses <code>geom_roc()</code></p>
<pre class="r"><code>library(plotROC)
ROCplot&lt;-ggplot(data,aes(d=disease,m=predictor))+geom_roc(n.cuts=0)+coord_fixed()
ROCplot</code></pre>
<pre><code>## Warning in verify_d(data$d): D not labeled 0/1, assuming benign = 0 and
## malignant = 1!</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<pre class="r"><code>calc_auc(ROCplot)</code></pre>
<pre><code>## Warning in verify_d(data$d): D not labeled 0/1, assuming benign = 0 and
## malignant = 1!</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.9903196</code></pre>
<p>Just for comparison, here is the ROC curve from our logistic regression using only clump thickness as a predictor</p>
<pre class="r"><code>library(plotROC)
ROCplot&lt;-ggplot(data,aes(d=disease,m=clump_thickness))+geom_roc(n.cuts=0)+coord_fixed()
ROCplot</code></pre>
<pre><code>## Warning in verify_d(data$d): D not labeled 0/1, assuming benign = 0 and
## malignant = 1!</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<pre class="r"><code>calc_auc(ROCplot)</code></pre>
<pre><code>## Warning in verify_d(data$d): D not labeled 0/1, assuming benign = 0 and
## malignant = 1!</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.9098416</code></pre>
<p>Here is another package called <code>auctestr</code> which gives you standard errors as well (using the large-sample approximation above)</p>
<pre class="r"><code>#install.packages(&quot;auctestr&quot;)
library(auctestr)
AUC&lt;-.9903
se_auc(AUC,n1,n2)</code></pre>
<pre><code>## [1] 0.004480684</code></pre>
<pre class="r"><code>sqrt((AUC*(1-AUC)+(n1-1)*((AUC/(2-AUC))-AUC^2)+(n2-1)*((2*AUC^2)/(1+AUC)-AUC^2))/(n1*n2))</code></pre>
<pre><code>## [1] 0.004480684</code></pre>
<p>The package `pROC gives bootstrap SE estimates too</p>
<pre class="r"><code>#install.packages(&quot;pROC&quot;)
library(pROC)</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:plotROC&#39;:
## 
##     ggroc</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre class="r"><code>data$test_num&lt;-data$test%&gt;%as.numeric()-1

roc(response=data$y,predictor=data$prob_pred,print.auc=T,ci=T,plot=T)</code></pre>
<pre><code>## Setting levels: control = 0, case = 1</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<pre><code>## 
## Call:
## roc.default(response = data$y, predictor = data$prob_pred, ci = T,     plot = T, print.auc = T)
## 
## Data: data$prob_pred in 458 controls (data$y 0) &lt; 241 cases (data$y 1).
## Area under the curve: 0.9903
## 95% CI: 0.9848-0.9959 (DeLong)</code></pre>
<pre class="r"><code>ci.auc(response=data$y,predictor=data$prob_pred,method=&quot;bootstrap&quot;)</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre><code>## 95% CI: 0.9842-0.9953 (2000 stratified bootstrap replicates)</code></pre>
<p>I’m curious if we can replicate this bootstrap ourselves by resampling cases with replacement. Let’s try it!</p>
<pre class="r"><code>AUCboots&lt;-replicate(1000, {
  samp&lt;-data[sample(1:dim(data)[1],replace=T),]
  fit&lt;-glm(y~predictor,data=samp,family=binomial)
  samp$prob_pred&lt;-predict(fit,type=&quot;response&quot;)
  
  TPR&lt;-sapply(seq(0,1,.01),true_pos,samp)
  FPR&lt;-sapply(seq(0,1,.01),false_pos,samp)
  ROC1&lt;-data.frame(TPR,FPR,cutoff=seq(0,1,.01)) 

  #compute AUC
  ROC1&lt;-ROC1[order(-ROC1$cutoff),]
  widths&lt;-diff(ROC1$FPR)
  heights&lt;-vector()
  for(i in 1:100) heights[i]&lt;-ROC1$TPR[i]+ROC1$TPR[i+1]
  sum(heights*widths/2)%&gt;%round(4)
})

mean(AUCboots)</code></pre>
<pre><code>## [1] 0.9894112</code></pre>
<pre class="r"><code>sd(AUCboots)</code></pre>
<pre><code>## [1] 0.002633121</code></pre>
<pre class="r"><code>mean(AUCboots)+c(-1,1)*sd(AUCboots)</code></pre>
<pre><code>## [1] 0.9867781 0.9920443</code></pre>
</div>
<div id="some-other-metrics" class="section level3">
<h3>Some other metrics</h3>
<div id="positive-predictive-value-ppv-or-pv-aka-precision" class="section level4">
<h4>Positive predictive value (PPV or PV+, aka Precision)</h4>
<p>Another metric that is commonly calculated is the PPV, or positive predictive value. This is just what it soundsl like the predictive value of a positive test result, or the probability of actually having the disease given a positive test result. Therefore, it is simply <span class="math inline">\(p(disease \mid positive)=\frac{p(disease\ \&amp;\ positive)}{p(positive)}=\frac{TP}{TP+FP}\)</span>. In this case (from the confusion matrix), there are 227 TPs and 14 FPs, for a PPV of 94.19. Thus, if you get a positive test result, there is a 94% chance that you actually have the disease. In this case, the Precision is the same as the Sesitivity because FP=FN.</p>
<pre class="r"><code>with(data, table(test,disease))%&gt;%addmargins</code></pre>
<pre><code>##           disease
## test       malignant benign Sum
##   negative        14    444 458
##   positive       227     14 241
##   Sum            241    458 699</code></pre>
</div>
<div id="positivenegative-likelihood-ratios" class="section level4">
<h4>Positive/negative likelihood ratios</h4>
<p>These are simply TP/FP for the positive LR and FN/TN for negative LR. So in this case, <span class="math inline">\(LR_+=\frac{227}{14}=16.214\)</span> and $LR_-==.0315. Thus, true positives are 16.2 times as likely as false positives and false negatives are .0315 times as likely as true negatives.</p>
<p>The ratio of LR+ and LR- is the <strong>diagnostic odds ratio</strong>: it is the ratio of the odds of testing positive if you have the disease relative to the odds of testing positive if you do not have the disease</p>
<p><span class="math display">\[
DOR=\frac{LR_+}{LR_-}=\frac{TP/FP}{FN/TN}=\frac{TP/FN}{FP/TN}=\frac{odds(+|disease)}{odds(+|no\ disease)}=\frac{227/14}{14/444}=514.225
\]</span> So the odds of testing positive are 514 times greater if you have the disease than if you do not!</p>
<p>It can be shown that the logarithm of an odds ratio is approximately normal with a standard error of <span class="math inline">\(SE(ln OR)=\sqrt{\frac{1}{TP}+\frac{1}{FP}+\frac{1}{TN}+\frac{1}{FN}}\)</span>.</p>
<p>So we have <span class="math inline">\(\sqrt{\frac{1}{227}+\frac{1}{14}+\frac{1}{14}+\frac{1}{444}}=.3867\)</span>, so our 95% CI for <span class="math inline">\(lnOR\)</span> is</p>
<pre class="r"><code>log(514.225)+(c(-1.96,1.96)*(.3867))</code></pre>
<pre><code>## [1] 5.484729 7.000593</code></pre>
<p>And exponentiating to get back to the original scale,</p>
<pre class="r"><code>exp(c(5.485,7.001))</code></pre>
<pre><code>## [1]  241.0489 1097.7303</code></pre>
</div>
</div>
</div>
<div id="poisson-regression" class="section level2">
<h2>Poisson Regression</h2>
<p>In my Biostatistics course, students have to conduct their own research project, which requires data. Many students choose to collect their own data and often their response variable is count-based. For example, they might ask how many times per month a student eats at a restaurant. However, in this course we only cover up to multiple regression and we do not broach the topic of link functions at all. Just like the binary cases above, we can’t very well model count data using a linear model, since a line has negative y-values for certain x values. Counts have a minimum value of 0!</p>
<p>Just like a linear regression models the average outcome as <span class="math inline">\(\mu_i=\beta_0+\beta_1x_i\)</span>, we could try to model the average of the count data in the same way, <span class="math inline">\(\lambda_i=\beta_0+\beta_1x_i\)</span>, where <span class="math inline">\(\lambda\)</span> is the average (and variance) of a Poisson distribution, often called the “rate”. But this is linear, so to avoid the problem of negative values, we use a link function that maps a domain of <span class="math inline">\((-\infty,\infty)\)</span> to a range of <span class="math inline">\([0,\infty)\)</span>, the natural log.</p>
<p><span class="math display">\[
log(\lambda_i)=\beta_0+\beta_1x_i
\]</span></p>
<p>We are also assuming that our observed counts <span class="math inline">\(Y_i\)</span> come from a Poisson distribution with <span class="math inline">\(\lambda=\lambda_i\)</span> for a given <span class="math inline">\(x_i\)</span>. Note that there is no separate error term: this is because <span class="math inline">\(\lambda\)</span> determines both the mean and the variance of our response variable.</p>
<p>Below we have the famous Prussian horse kicks dataset, compiled by Ladislaus Bortkiewicz, the father of Poisson applications (see his book Das Gezets der kleinen Zahlen, or <em>the Law of Small Numbers</em>). This dataset gives the number of personnel in the Prussian army killed by horse kicks per year from 1875 to 1895 from 10 different corps. Each corps is given its own column, so we will need to rearrange a bit.</p>
<pre class="r"><code>prussian&lt;-read.table(&quot;http://www.randomservices.org/random/data/HorseKicks.txt&quot;,header = T)

prussian&lt;-gather(prussian,Corps,Kicks,-Year)
prussian$Corps&lt;-as.factor(prussian$Corps)
prussian</code></pre>
<pre><code>##     Year Corps Kicks
## 1   1875    GC     0
## 2   1876    GC     2
## 3   1877    GC     2
## 4   1878    GC     1
## 5   1879    GC     0
## 6   1880    GC     0
## 7   1881    GC     1
## 8   1882    GC     1
## 9   1883    GC     0
## 10  1884    GC     3
## 11  1885    GC     0
## 12  1886    GC     2
## 13  1887    GC     1
## 14  1888    GC     0
## 15  1889    GC     0
## 16  1890    GC     1
## 17  1891    GC     0
## 18  1892    GC     1
## 19  1893    GC     0
## 20  1894    GC     1
## 21  1875    C1     0
## 22  1876    C1     0
## 23  1877    C1     0
## 24  1878    C1     2
## 25  1879    C1     0
## 26  1880    C1     3
## 27  1881    C1     0
## 28  1882    C1     2
## 29  1883    C1     0
## 30  1884    C1     0
## 31  1885    C1     0
## 32  1886    C1     1
## 33  1887    C1     1
## 34  1888    C1     1
## 35  1889    C1     0
## 36  1890    C1     2
## 37  1891    C1     0
## 38  1892    C1     3
## 39  1893    C1     1
## 40  1894    C1     0
## 41  1875    C2     0
## 42  1876    C2     0
## 43  1877    C2     0
## 44  1878    C2     2
## 45  1879    C2     0
## 46  1880    C2     2
## 47  1881    C2     0
## 48  1882    C2     0
## 49  1883    C2     1
## 50  1884    C2     1
## 51  1885    C2     0
## 52  1886    C2     0
## 53  1887    C2     2
## 54  1888    C2     1
## 55  1889    C2     1
## 56  1890    C2     0
## 57  1891    C2     0
## 58  1892    C2     2
## 59  1893    C2     0
## 60  1894    C2     0
## 61  1875    C3     0
## 62  1876    C3     0
## 63  1877    C3     0
## 64  1878    C3     1
## 65  1879    C3     1
## 66  1880    C3     1
## 67  1881    C3     2
## 68  1882    C3     0
## 69  1883    C3     2
## 70  1884    C3     0
## 71  1885    C3     0
## 72  1886    C3     0
## 73  1887    C3     1
## 74  1888    C3     0
## 75  1889    C3     1
## 76  1890    C3     2
## 77  1891    C3     1
## 78  1892    C3     0
## 79  1893    C3     0
## 80  1894    C3     0
## 81  1875    C4     0
## 82  1876    C4     1
## 83  1877    C4     0
## 84  1878    C4     1
## 85  1879    C4     1
## 86  1880    C4     1
## 87  1881    C4     1
## 88  1882    C4     0
## 89  1883    C4     0
## 90  1884    C4     0
## 91  1885    C4     0
## 92  1886    C4     1
## 93  1887    C4     0
## 94  1888    C4     0
## 95  1889    C4     0
## 96  1890    C4     0
## 97  1891    C4     1
## 98  1892    C4     1
## 99  1893    C4     0
## 100 1894    C4     0
## 101 1875    C5     0
## 102 1876    C5     0
## 103 1877    C5     0
## 104 1878    C5     0
## 105 1879    C5     2
## 106 1880    C5     1
## 107 1881    C5     0
## 108 1882    C5     0
## 109 1883    C5     1
## 110 1884    C5     0
## 111 1885    C5     0
## 112 1886    C5     1
## 113 1887    C5     0
## 114 1888    C5     1
## 115 1889    C5     1
## 116 1890    C5     1
## 117 1891    C5     1
## 118 1892    C5     1
## 119 1893    C5     1
## 120 1894    C5     0
## 121 1875    C6     0
## 122 1876    C6     0
## 123 1877    C6     1
## 124 1878    C6     0
## 125 1879    C6     2
## 126 1880    C6     0
## 127 1881    C6     0
## 128 1882    C6     1
## 129 1883    C6     2
## 130 1884    C6     0
## 131 1885    C6     1
## 132 1886    C6     1
## 133 1887    C6     3
## 134 1888    C6     1
## 135 1889    C6     1
## 136 1890    C6     1
## 137 1891    C6     0
## 138 1892    C6     3
## 139 1893    C6     0
## 140 1894    C6     0
## 141 1875    C7     1
## 142 1876    C7     0
## 143 1877    C7     1
## 144 1878    C7     0
## 145 1879    C7     0
## 146 1880    C7     0
## 147 1881    C7     1
## 148 1882    C7     0
## 149 1883    C7     1
## 150 1884    C7     1
## 151 1885    C7     0
## 152 1886    C7     0
## 153 1887    C7     2
## 154 1888    C7     0
## 155 1889    C7     0
## 156 1890    C7     2
## 157 1891    C7     1
## 158 1892    C7     0
## 159 1893    C7     2
## 160 1894    C7     0
## 161 1875    C8     1
## 162 1876    C8     0
## 163 1877    C8     0
## 164 1878    C8     0
## 165 1879    C8     1
## 166 1880    C8     0
## 167 1881    C8     0
## 168 1882    C8     1
## 169 1883    C8     0
## 170 1884    C8     0
## 171 1885    C8     0
## 172 1886    C8     0
## 173 1887    C8     1
## 174 1888    C8     0
## 175 1889    C8     0
## 176 1890    C8     0
## 177 1891    C8     1
## 178 1892    C8     1
## 179 1893    C8     0
## 180 1894    C8     1
## 181 1875    C9     0
## 182 1876    C9     0
## 183 1877    C9     0
## 184 1878    C9     0
## 185 1879    C9     0
## 186 1880    C9     2
## 187 1881    C9     1
## 188 1882    C9     1
## 189 1883    C9     1
## 190 1884    C9     0
## 191 1885    C9     2
## 192 1886    C9     1
## 193 1887    C9     1
## 194 1888    C9     0
## 195 1889    C9     1
## 196 1890    C9     2
## 197 1891    C9     0
## 198 1892    C9     1
## 199 1893    C9     0
## 200 1894    C9     0
## 201 1875   C10     0
## 202 1876   C10     0
## 203 1877   C10     1
## 204 1878   C10     1
## 205 1879   C10     0
## 206 1880   C10     1
## 207 1881   C10     0
## 208 1882   C10     2
## 209 1883   C10     0
## 210 1884   C10     2
## 211 1885   C10     0
## 212 1886   C10     0
## 213 1887   C10     0
## 214 1888   C10     0
## 215 1889   C10     2
## 216 1890   C10     1
## 217 1891   C10     3
## 218 1892   C10     0
## 219 1893   C10     1
## 220 1894   C10     1
## 221 1875   C11     0
## 222 1876   C11     0
## 223 1877   C11     0
## 224 1878   C11     0
## 225 1879   C11     2
## 226 1880   C11     4
## 227 1881   C11     0
## 228 1882   C11     1
## 229 1883   C11     3
## 230 1884   C11     0
## 231 1885   C11     1
## 232 1886   C11     1
## 233 1887   C11     1
## 234 1888   C11     1
## 235 1889   C11     2
## 236 1890   C11     1
## 237 1891   C11     3
## 238 1892   C11     1
## 239 1893   C11     3
## 240 1894   C11     1
## 241 1875   C14     1
## 242 1876   C14     1
## 243 1877   C14     2
## 244 1878   C14     1
## 245 1879   C14     1
## 246 1880   C14     3
## 247 1881   C14     0
## 248 1882   C14     4
## 249 1883   C14     0
## 250 1884   C14     1
## 251 1885   C14     0
## 252 1886   C14     3
## 253 1887   C14     2
## 254 1888   C14     1
## 255 1889   C14     0
## 256 1890   C14     2
## 257 1891   C14     1
## 258 1892   C14     1
## 259 1893   C14     0
## 260 1894   C14     0
## 261 1875   C15     0
## 262 1876   C15     1
## 263 1877   C15     0
## 264 1878   C15     0
## 265 1879   C15     0
## 266 1880   C15     0
## 267 1881   C15     0
## 268 1882   C15     1
## 269 1883   C15     0
## 270 1884   C15     1
## 271 1885   C15     1
## 272 1886   C15     0
## 273 1887   C15     0
## 274 1888   C15     0
## 275 1889   C15     2
## 276 1890   C15     2
## 277 1891   C15     0
## 278 1892   C15     0
## 279 1893   C15     0
## 280 1894   C15     0</code></pre>
<p>Good! Notice that most of the Kicks per year are close to zero. For a Poisson distribution, recall that the pmf is given as</p>
<p><span class="math display">\[
P(y|\lambda)=\frac{\lambda^ye^{-\lambda}}{y!}
\]</span> And <span class="math inline">\(E(y)=\lambda\)</span> and <span class="math inline">\(Var(Y)=\lambda\)</span>. Thus, using <code>mean(y)</code> is a good estimate for <span class="math inline">\(\lambda\)</span> (indeed, it can be shown that it is the maximum likelihood estimate). Let’s quickly do this. There are 280 observations, and assuming they arise from a poisson distribution, we have the following likelihood function, which we take the log of for convenience, then take the deriviative with respect to <span class="math inline">\(\lambda\)</span> so we can solve for the critical point (in this case, the maximum of the likelihood function).</p>
<p><span class="math display">\[
\begin{aligned}
L(\lambda,y)&amp;=\prod_{i=1}^{280}\frac{\lambda^{y_i}e^{-\lambda}}{y_i!}\\
log(L(\lambda,y))&amp;=-280\lambda+log(\lambda)\sum_{i=1}^{280} y-\sum log(y!)\\
\frac{d}{d\lambda}log(L(\lambda,y))&amp;=-280+\frac1\lambda\sum_{i=1}^{280}y \phantom{xxxxx}\text{set equal to zero to find maximum}\\
0&amp;:=-280+\frac1\lambda\sum_{i=1}^{280}y\\
\rightarrow \lambda&amp;=\sum_{i=1}^{280}y/280\\
\end{aligned}
\]</span> Thus, the maximum probability occurs when we set <span class="math inline">\(\lambda\)</span> to the mean of the data <span class="math inline">\(y\)</span>, which is exactly what we see.</p>
<pre class="r"><code>loglik&lt;-function(lambda){sum(log(lambda^prussian$Kicks*exp(-lambda)/factorial(prussian$Kicks)))}
loglik&lt;-sapply(seq(.1,5,.1),FUN=loglik)

plot(seq(.1,5,.1),loglik,type=&quot;l&quot;)
abline(v=.7)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<pre class="r"><code>mean(prussian$Kicks)</code></pre>
<pre><code>## [1] 0.7</code></pre>
<pre class="r"><code>prussian%&gt;%ggplot(aes(Kicks))+geom_histogram(aes(y=..density..),bins=5)+stat_function(geom=&quot;point&quot;,fun=dpois,n=5,args=list(lambda=.7),color=&quot;red&quot;,size=3)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>These data clearly follow a Poisson distribution (as do most low-count data). Let’s test the null hypothesis that the observed counts are not different from the counts expected under a poisson distribution with a <span class="math inline">\(\lambda=.7\)</span>. We can get our observed counts by simply tabling our response variable. The expected couns of 0, 1, 2, 3, and 4 in a sample of 280 can be calculated using the poisson pmf <span class="math inline">\(p(y|\lambda=.7)=\frac{.7^ye^{-.7}}{y!}\)</span> and the total sample size.</p>
<pre class="r"><code>obs&lt;-table(prussian$Kicks)
exp&lt;-dpois(0:4,.7)*280
cbind(obs,exp)</code></pre>
<pre><code>##   obs        exp
## 0 144 139.043885
## 1  91  97.330720
## 2  32  34.065752
## 3  11   7.948675
## 4   2   1.391018</code></pre>
<p>Now we just have to calculate a chi-squared test statistic quantifying how far these counts are from each other and see if it is large enough to warrant rejecting the null hypothesis that they were generated with this probabilities.</p>
<pre class="r"><code>chisq&lt;-sum((obs-exp)^2/exp)
pchisq(chisq,df = 3,lower.tail = F)</code></pre>
<pre><code>## [1] 0.5415359</code></pre>
<p>Fail to reject: we don’t have evidence that the data differ from this expected distribution.</p>
<div id="poisson-regression-1" class="section level4">
<h4>Poisson Regression</h4>
<p>For Poisson random variables, the variance is equal to the mean. Let’s look within groups of years and see if this holds</p>
<pre class="r"><code>prussian$Group&lt;-cut(prussian$Year,breaks = seq(1875,1895,5),include.lowest = T)
prussian%&gt;%ggplot(aes(Kicks))+geom_histogram(bins=5)+facet_wrap(~Group)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<pre class="r"><code>prussian%&gt;%group_by(Group)%&gt;%summarize(mean(Kicks),var(Kicks),n())</code></pre>
<pre><code>## # A tibble: 4 x 4
##   Group       `mean(Kicks)` `var(Kicks)` `n()`
##   &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;
## 1 [1875,1880]         0.619        0.769    84
## 2 (1880,1885]         0.643        0.784    70
## 3 (1885,1890]         0.857        0.675    70
## 4 (1890,1895]         0.696        0.833    56</code></pre>
<p>This looks OK. Another assumption is the <span class="math inline">\(log(\lambda_i)\)</span> is a linear function of Year, since we are modeling it as such. Let’s look at the average number of kicks per year and see if it is a linear function of year.</p>
<pre class="r"><code>prussian%&gt;%group_by(Year)%&gt;%summarize(Kicks=mean(Kicks))%&gt;%ggplot(aes(Year,log(Kicks),group))+geom_point()+geom_smooth()</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>Hmm, it doesn’t look especially linear. Perhaps including a quadratic term is appropriate. Still, the assumption is about the true means, not the empirical means. Let’s go ahead and run our regressions.</p>
<p>Now, a Poisson regression differs from a regular regression in two ways: the errors should follow a Poisson distribution, and the link function is logarithmic. Let’s examine the effect of Year just to use a continuous predictor.</p>
<pre class="r"><code>fit3&lt;-glm(Kicks~Year,data=prussian,family=&quot;poisson&quot;)
summary(fit3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Year, family = &quot;poisson&quot;, data = prussian)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.2897  -1.1742  -1.0792   0.3997   2.8187  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -35.72380   23.43349  -1.524    0.127
## Year          0.01876    0.01243   1.510    0.131
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 323.23  on 279  degrees of freedom
## Residual deviance: 320.94  on 278  degrees of freedom
## AIC: 630.02
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Because of the (log) link function, the coefficient estimate will need to be transformed via exponentiation. Here, <span class="math inline">\(e^{.019}=1.02\)</span>, so every additional year increases the expected number of horse-kick deaths by a <em>factor of 1.02</em>, or a yearly increase of 2%, which is not significant. Note that the increase is multiplicative rather than additive as in linear regression.</p>
<p>To get our 95% CI, we take our estimate <span class="math inline">\(\pm 1.96 SE\)</span> and then exponentiate. So here we have <span class="math inline">\(.01875 \pm 1.96*.1243=[-.0055,.0432]\)</span>, and we exponentiate each limit, <span class="math inline">\([.9945, 1.0441]\)</span>. It contains 1, so no effect!</p>
<pre class="r"><code>prussian$pred&lt;-predict(fit3,type=&quot;response&quot;)

prussian%&gt;%ggplot(aes(Year,Kicks))+geom_point()+geom_line(aes(y=pred))</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<pre class="r"><code>breaks &lt;- seq(min(prussian$Year), max(prussian$Year), len=8)
prussian$section &lt;- cut(prussian$Year, breaks)
dens &lt;- do.call(rbind, lapply(split(prussian, prussian$section), function(x) {
    res &lt;- data.frame(y=0:5,x= max(x$Year)-3*dpois(0:5,lambda=mean(x$Kicks)))
    res
}))
dens$section &lt;- rep(levels(prussian$section), each=6)

prussian%&gt;%ggplot(aes(Year,Kicks))+geom_point(alpha=.5)+geom_line(aes(y=pred))+
 # geom_path(data=dens, aes(x, y, group=interaction(section,type), color=type), lwd=1)+
  geom_step(data=dens,aes(x,y,group=section),lwd=1,color=&#39;red&#39;,direction=&quot;vh&quot;)+
  theme_bw() + theme(legend.position=&quot;none&quot;)+
  geom_vline(xintercept=breaks, lty=2,color=&#39;gray50&#39;)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-51-2.png" width="672" /></p>
<p>We can see the same thing by comparing this model to the null model and seeing if we significantly decrease deviance (we get the same p-value).</p>
<pre class="r"><code>nullfit&lt;-glm(Kicks~1, data=prussian, family=&quot;poisson&quot;)
anova(nullfit,fit3,test=&quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Kicks ~ 1
## Model 2: Kicks ~ Year
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1       279     323.23                     
## 2       278     320.94  1   2.2866   0.1305</code></pre>
<p>Because of our quadratic looking plot above, let’s add a term that let’s Year affect Kicks quadratically</p>
<pre class="r"><code>fit3quad&lt;-glm(Kicks~Year+I(Year^2),data=prussian,family=&quot;poisson&quot;)
summary(fit3quad)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Year + I(Year^2), family = &quot;poisson&quot;, data = prussian)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3175  -1.2046  -0.8679   0.3841   2.7584  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -2.376e+04  9.246e+03  -2.570   0.0102 *
## Year         2.520e+01  9.812e+00   2.568   0.0102 *
## I(Year^2)   -6.679e-03  2.603e-03  -2.566   0.0103 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 323.23  on 279  degrees of freedom
## Residual deviance: 314.00  on 277  degrees of freedom
## AIC: 625.08
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>anova(fit3,fit3quad,test=&quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Kicks ~ Year
## Model 2: Kicks ~ Year + I(Year^2)
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)   
## 1       278     320.94                        
## 2       277     314.00  1   6.9469 0.008397 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Looks like it does: the quadratic model definitely fits better. Our model is <span class="math inline">\(log(Kicks)=-2376+25.2Year-.0067Year^2\)</span>, and maximizing this function shows that the year with the most deaths was <span class="math inline">\(\frac{25.2}{.0135]=1866.67\)</span></p>
<p>We could also examine the differences among corps:</p>
<pre class="r"><code>fit4&lt;-glm(Kicks~Corps,data=prussian,family=&quot;poisson&quot;)
summary(fit4)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Corps, family = &quot;poisson&quot;, data = prussian)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5811  -1.0955  -0.8367   0.5438   2.0079  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -2.231e-01  2.500e-01  -0.893   0.3721  
## CorpsC10    -6.454e-02  3.594e-01  -0.180   0.8575  
## CorpsC11     4.463e-01  3.201e-01   1.394   0.1633  
## CorpsC14     4.055e-01  3.227e-01   1.256   0.2090  
## CorpsC15    -6.931e-01  4.330e-01  -1.601   0.1094  
## CorpsC2     -2.877e-01  3.819e-01  -0.753   0.4512  
## CorpsC3     -2.877e-01  3.819e-01  -0.753   0.4512  
## CorpsC4     -6.931e-01  4.330e-01  -1.601   0.1094  
## CorpsC5     -3.747e-01  3.917e-01  -0.957   0.3387  
## CorpsC6      6.062e-02  3.483e-01   0.174   0.8618  
## CorpsC7     -2.877e-01  3.819e-01  -0.753   0.4512  
## CorpsC8     -8.267e-01  4.532e-01  -1.824   0.0681 .
## CorpsC9     -2.076e-01  3.734e-01  -0.556   0.5781  
## CorpsGC     -4.072e-09  3.535e-01   0.000   1.0000  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 323.23  on 279  degrees of freedom
## Residual deviance: 297.09  on 266  degrees of freedom
## AIC: 630.17
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>No corps differ significantly from C1, but there do appear to be some differences. For example, a person in corps C11 is <span class="math inline">\(e^{4.46}=1.56\)</span>, or 56% more likely to die from horse kicks than C1, but corps C8 is <span class="math inline">\(e^{-.827}=.44\)</span>, or about 66% <em>less</em> likely to die from horse kicks than C1. We should be mindful of multiple comparisons, but let’s see if C11 differs from C8 by releveling</p>
<pre class="r"><code>prussian$Corps&lt;-relevel(prussian$Corps,ref=&quot;C8&quot;)

fit4&lt;-glm(Kicks~Corps,data=prussian,family=&quot;poisson&quot;)
summary(fit4)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Corps, family = &quot;poisson&quot;, data = prussian)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5811  -1.0955  -0.8367   0.5438   2.0079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  -1.0498     0.3780  -2.778  0.00548 **
## CorpsC1       0.8267     0.4532   1.824  0.06811 . 
## CorpsC10      0.7621     0.4577   1.665  0.09590 . 
## CorpsC11      1.2730     0.4276   2.977  0.00291 **
## CorpsC14      1.2321     0.4296   2.868  0.00413 **
## CorpsC15      0.1335     0.5175   0.258  0.79639   
## CorpsC2       0.5390     0.4756   1.133  0.25707   
## CorpsC3       0.5390     0.4756   1.133  0.25707   
## CorpsC4       0.1335     0.5175   0.258  0.79640   
## CorpsC5       0.4520     0.4835   0.935  0.34987   
## CorpsC6       0.8873     0.4491   1.976  0.04818 * 
## CorpsC7       0.5390     0.4756   1.133  0.25707   
## CorpsC9       0.6190     0.4688   1.320  0.18668   
## CorpsGC       0.8267     0.4532   1.824  0.06811 . 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 323.23  on 279  degrees of freedom
## Residual deviance: 297.09  on 266  degrees of freedom
## AIC: 630.17
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Wow, it looks like lots of Corps differ from C8: C8 had surprisingly few horse deaths! Since we have a factor variable, to test if the overall factor is significant, let’s use deviances to compare this model to the null model and test the overall effect of Corps.</p>
<pre class="r"><code>anova(nullfit,fit4,test=&quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Kicks ~ 1
## Model 2: Kicks ~ Corps
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  
## 1       279     323.23                       
## 2       266     297.09 13   26.137   0.0163 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="offsets-and-overdispersion" class="section level3">
<h3>Offsets and Overdispersion</h3>
<p>If you are dealing with counts across groups whose sizes are different, you must explicitly take this into account. For example, assume that corps sizes vary. I am just going to make up some corps sizes for the purposes of illustration. If <span class="math inline">\(\lambda\)</span> is the mean number of kicks per year, we can adjust for corps size by including its log as an offset.</p>
<pre class="r"><code>prussian&lt;-prussian%&gt;%mutate(Size=round(runif(n=280,min = 5000, max=10000)))
prussian%&gt;%dplyr::select(Year,Corps,Kicks,Size)</code></pre>
<pre><code>##     Year Corps Kicks Size
## 1   1875    GC     0 9196
## 2   1876    GC     2 5659
## 3   1877    GC     2 8478
## 4   1878    GC     1 5761
## 5   1879    GC     0 5181
## 6   1880    GC     0 9432
## 7   1881    GC     1 5844
## 8   1882    GC     1 8881
## 9   1883    GC     0 7070
## 10  1884    GC     3 9358
## 11  1885    GC     0 5502
## 12  1886    GC     2 5453
## 13  1887    GC     1 6458
## 14  1888    GC     0 8971
## 15  1889    GC     0 7585
## 16  1890    GC     1 5783
## 17  1891    GC     0 5591
## 18  1892    GC     1 8226
## 19  1893    GC     0 7789
## 20  1894    GC     1 7128
## 21  1875    C1     0 8914
## 22  1876    C1     0 5399
## 23  1877    C1     0 5171
## 24  1878    C1     2 5366
## 25  1879    C1     0 7962
## 26  1880    C1     3 9436
## 27  1881    C1     0 6145
## 28  1882    C1     2 5147
## 29  1883    C1     0 6493
## 30  1884    C1     0 8363
## 31  1885    C1     0 9441
## 32  1886    C1     1 6510
## 33  1887    C1     1 5614
## 34  1888    C1     1 9176
## 35  1889    C1     0 6831
## 36  1890    C1     2 5139
## 37  1891    C1     0 8813
## 38  1892    C1     3 6277
## 39  1893    C1     1 9977
## 40  1894    C1     0 5486
## 41  1875    C2     0 6051
## 42  1876    C2     0 6688
## 43  1877    C2     0 9007
## 44  1878    C2     2 5177
## 45  1879    C2     0 8301
## 46  1880    C2     2 9994
## 47  1881    C2     0 6252
## 48  1882    C2     0 6484
## 49  1883    C2     1 5729
## 50  1884    C2     1 6610
## 51  1885    C2     0 7722
## 52  1886    C2     0 7112
## 53  1887    C2     2 5665
## 54  1888    C2     1 5465
## 55  1889    C2     1 7069
## 56  1890    C2     0 6545
## 57  1891    C2     0 7059
## 58  1892    C2     2 5085
## 59  1893    C2     0 6416
## 60  1894    C2     0 6243
## 61  1875    C3     0 9582
## 62  1876    C3     0 7350
## 63  1877    C3     0 9779
## 64  1878    C3     1 6804
## 65  1879    C3     1 5561
## 66  1880    C3     1 8318
## 67  1881    C3     2 8940
## 68  1882    C3     0 6456
## 69  1883    C3     2 9140
## 70  1884    C3     0 7594
## 71  1885    C3     0 8140
## 72  1886    C3     0 9745
## 73  1887    C3     1 8416
## 74  1888    C3     0 7917
## 75  1889    C3     1 5689
## 76  1890    C3     2 5318
## 77  1891    C3     1 7120
## 78  1892    C3     0 5047
## 79  1893    C3     0 6188
## 80  1894    C3     0 6079
## 81  1875    C4     0 5154
## 82  1876    C4     1 5967
## 83  1877    C4     0 5229
## 84  1878    C4     1 5965
## 85  1879    C4     1 8273
## 86  1880    C4     1 5840
## 87  1881    C4     1 8266
## 88  1882    C4     0 6489
## 89  1883    C4     0 6502
## 90  1884    C4     0 9960
## 91  1885    C4     0 6825
## 92  1886    C4     1 8240
## 93  1887    C4     0 8558
## 94  1888    C4     0 7657
## 95  1889    C4     0 7316
## 96  1890    C4     0 5163
## 97  1891    C4     1 5093
## 98  1892    C4     1 9757
## 99  1893    C4     0 5298
## 100 1894    C4     0 8752
## 101 1875    C5     0 7220
## 102 1876    C5     0 7664
## 103 1877    C5     0 7180
## 104 1878    C5     0 9514
## 105 1879    C5     2 5562
## 106 1880    C5     1 9792
## 107 1881    C5     0 6202
## 108 1882    C5     0 8407
## 109 1883    C5     1 6026
## 110 1884    C5     0 8454
## 111 1885    C5     0 7850
## 112 1886    C5     1 9282
## 113 1887    C5     0 6339
## 114 1888    C5     1 6684
## 115 1889    C5     1 6599
## 116 1890    C5     1 5820
## 117 1891    C5     1 5734
## 118 1892    C5     1 5395
## 119 1893    C5     1 7621
## 120 1894    C5     0 9380
## 121 1875    C6     0 5040
## 122 1876    C6     0 6316
## 123 1877    C6     1 8690
## 124 1878    C6     0 5486
## 125 1879    C6     2 5168
## 126 1880    C6     0 8118
## 127 1881    C6     0 9100
## 128 1882    C6     1 7725
## 129 1883    C6     2 8808
## 130 1884    C6     0 7054
## 131 1885    C6     1 5179
## 132 1886    C6     1 7205
## 133 1887    C6     3 6449
## 134 1888    C6     1 6976
## 135 1889    C6     1 9836
## 136 1890    C6     1 6966
## 137 1891    C6     0 7529
## 138 1892    C6     3 5554
## 139 1893    C6     0 9121
## 140 1894    C6     0 8817
## 141 1875    C7     1 7627
## 142 1876    C7     0 8320
## 143 1877    C7     1 8448
## 144 1878    C7     0 6070
## 145 1879    C7     0 5307
## 146 1880    C7     0 5554
## 147 1881    C7     1 8307
## 148 1882    C7     0 6316
## 149 1883    C7     1 9258
## 150 1884    C7     1 6248
## 151 1885    C7     0 6132
## 152 1886    C7     0 6228
## 153 1887    C7     2 9844
## 154 1888    C7     0 9511
## 155 1889    C7     0 7556
## 156 1890    C7     2 7472
## 157 1891    C7     1 8709
## 158 1892    C7     0 5338
## 159 1893    C7     2 5089
## 160 1894    C7     0 6306
## 161 1875    C8     1 7966
## 162 1876    C8     0 6652
## 163 1877    C8     0 7080
## 164 1878    C8     0 6185
## 165 1879    C8     1 8978
## 166 1880    C8     0 9743
## 167 1881    C8     0 7579
## 168 1882    C8     1 9486
## 169 1883    C8     0 6918
## 170 1884    C8     0 7135
## 171 1885    C8     0 9633
## 172 1886    C8     0 5175
## 173 1887    C8     1 9688
## 174 1888    C8     0 9988
## 175 1889    C8     0 7811
## 176 1890    C8     0 9772
## 177 1891    C8     1 8174
## 178 1892    C8     1 7435
## 179 1893    C8     0 5853
## 180 1894    C8     1 5010
## 181 1875    C9     0 8091
## 182 1876    C9     0 7934
## 183 1877    C9     0 7604
## 184 1878    C9     0 7722
## 185 1879    C9     0 9760
## 186 1880    C9     2 9942
## 187 1881    C9     1 6355
## 188 1882    C9     1 8418
## 189 1883    C9     1 5708
## 190 1884    C9     0 5035
## 191 1885    C9     2 7851
## 192 1886    C9     1 7413
## 193 1887    C9     1 7714
## 194 1888    C9     0 7262
## 195 1889    C9     1 9574
## 196 1890    C9     2 6318
## 197 1891    C9     0 5141
## 198 1892    C9     1 9795
## 199 1893    C9     0 8245
## 200 1894    C9     0 8567
## 201 1875   C10     0 5575
## 202 1876   C10     0 7239
## 203 1877   C10     1 8828
## 204 1878   C10     1 7882
## 205 1879   C10     0 5974
## 206 1880   C10     1 6082
## 207 1881   C10     0 8687
## 208 1882   C10     2 5460
## 209 1883   C10     0 5925
## 210 1884   C10     2 9974
## 211 1885   C10     0 6267
## 212 1886   C10     0 5658
## 213 1887   C10     0 5181
## 214 1888   C10     0 7457
## 215 1889   C10     2 9676
## 216 1890   C10     1 5874
## 217 1891   C10     3 6471
## 218 1892   C10     0 7929
## 219 1893   C10     1 7224
## 220 1894   C10     1 5837
## 221 1875   C11     0 8998
## 222 1876   C11     0 7030
## 223 1877   C11     0 5025
## 224 1878   C11     0 5434
## 225 1879   C11     2 9560
## 226 1880   C11     4 9672
## 227 1881   C11     0 5500
## 228 1882   C11     1 7054
## 229 1883   C11     3 9018
## 230 1884   C11     0 9358
## 231 1885   C11     1 7905
## 232 1886   C11     1 9917
## 233 1887   C11     1 5884
## 234 1888   C11     1 5719
## 235 1889   C11     2 5552
## 236 1890   C11     1 8077
## 237 1891   C11     3 7935
## 238 1892   C11     1 8770
## 239 1893   C11     3 9467
## 240 1894   C11     1 8060
## 241 1875   C14     1 7152
## 242 1876   C14     1 9266
## 243 1877   C14     2 7395
## 244 1878   C14     1 8048
## 245 1879   C14     1 5758
## 246 1880   C14     3 7483
## 247 1881   C14     0 7828
## 248 1882   C14     4 5184
## 249 1883   C14     0 6976
## 250 1884   C14     1 8390
## 251 1885   C14     0 8006
## 252 1886   C14     3 7427
## 253 1887   C14     2 8452
## 254 1888   C14     1 7275
## 255 1889   C14     0 5183
## 256 1890   C14     2 7226
## 257 1891   C14     1 5043
## 258 1892   C14     1 9791
## 259 1893   C14     0 5744
## 260 1894   C14     0 8555
## 261 1875   C15     0 7395
## 262 1876   C15     1 8238
## 263 1877   C15     0 8879
## 264 1878   C15     0 5495
## 265 1879   C15     0 5552
## 266 1880   C15     0 5504
## 267 1881   C15     0 7268
## 268 1882   C15     1 5040
## 269 1883   C15     0 6796
## 270 1884   C15     1 7901
## 271 1885   C15     1 7626
## 272 1886   C15     0 5175
## 273 1887   C15     0 7301
## 274 1888   C15     0 9282
## 275 1889   C15     2 6781
## 276 1890   C15     2 8464
## 277 1891   C15     0 7462
## 278 1892   C15     0 5810
## 279 1893   C15     0 9923
## 280 1894   C15     0 8077</code></pre>
<pre class="r"><code>fitoffset&lt;-glm(Kicks~Year,data=prussian,family=&quot;poisson&quot;, offset=log(Size))
summary(fitoffset)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Year, family = &quot;poisson&quot;, data = prussian, 
##     offset = log(Size))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4954  -1.1617  -0.9272   0.4772   3.1685  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -45.73589   23.49987  -1.946   0.0516 .
## Year          0.01936    0.01247   1.553   0.1205  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 328.09  on 279  degrees of freedom
## Residual deviance: 325.67  on 278  degrees of freedom
## AIC: 634.75
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Now we have estimates that are correctly adjusted for corps size.</p>
<div id="overdispersion" class="section level4">
<h4>Overdispersion</h4>
<p>Sometimes it will happen that there will be more variation in the response variable than is expected from the Poisson distribution. Recall that we expect <span class="math inline">\(E(Y)=Var(Y)\)</span>. Let <span class="math inline">\(\phi\)</span> be an overdispersion parameter such that <span class="math inline">\(E(Y)=\phi Var(Y)\)</span>. If <span class="math inline">\(\phi=1\)</span>, there is no overdispersion, but if it is greater than 1, there is overdispersion. Overdispersion is problematic in that if it exists, the standard errors will be too small, leading to a greater type I error rate. We can get an estimate of the overdispersion by dividing the model deviance (sum of squared pearson residuals) by its degrees of freedom,</p>
<pre class="r"><code>sum(residuals(fitoffset,&quot;pearson&quot;)^2)/df.residual(fitoffset)</code></pre>
<pre><code>## [1] 1.153363</code></pre>
<pre class="r"><code>sqrt(1.1628)</code></pre>
<pre><code>## [1] 1.078332</code></pre>
<p>We would need to multiply our standard errors by <span class="math inline">\(\sqrt{\hat \phi}\)</span> to correct for overdispersion, so <span class="math inline">\(.01247\times \sqrt{1.1628}=.01247*1.0783=.01345\)</span>. We can get this directly by setting <code>family=&quot;quasipoisson&quot;</code>, as below. Note that tests are now based on the <em>t</em> distribution.</p>
<pre class="r"><code>fitquasi&lt;-glm(Kicks~Year,data=prussian,family=&quot;quasipoisson&quot;, offset=log(Size))
summary(fitquasi)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Year, family = &quot;quasipoisson&quot;, data = prussian, 
##     offset = log(Size))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4954  -1.1617  -0.9272   0.4772   3.1685  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -45.73589   25.23824  -1.812    0.071 .
## Year          0.01936    0.01339   1.446    0.149  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 1.153419)
## 
##     Null deviance: 328.09  on 279  degrees of freedom
## Residual deviance: 325.67  on 278  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Perhaps a simpler technique is to use robust standard errors.</p>
<p>Another common way of handling overdispersion is to use a negative binomial model, NegBinom(. You might know this distribution as modeling the number of (Bernoulli) trials before a certain number of failures occurs, but mathematically it is a Poisson model where <span class="math inline">\(\lamda\)</span> is itself random, following a gamma distribution, <span class="math inline">\(\lambda\sim Gamma(r, \frac{1-p}{p})\)</span>. So instead of <span class="math inline">\(Y\sim Poisson(\lambda)\)</span>, we model our responses as <span class="math inline">\(Y\sim NegBinom(r,p)\)</span> where <span class="math inline">\(E(Y)=pr/(1-p)=\mu\)</span> and <span class="math inline">\(Var(Y)=\mu+\frac{\mu^2}{r}\)</span>, where <span class="math inline">\(\frac{\mu^2}{r}\)</span> is our overdispersion parameter.</p>
<pre class="r"><code>fitnb&lt;-glm.nb(Kicks~Year,data=prussian, weights=offset(log(Size)))
summary(fitnb)</code></pre>
<pre><code>## 
## Call:
## glm.nb(formula = Kicks ~ Year, data = prussian, weights = offset(log(Size)), 
##     init.theta = 8.355198954, link = log)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -3.808  -3.429  -3.139   1.124   7.906  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -35.460285   8.188981  -4.330 1.49e-05 ***
## Year          0.018625   0.004344   4.287 1.81e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Negative Binomial(8.3552) family taken to be 1)
## 
##     Null deviance: 2665.8  on 279  degrees of freedom
## Residual deviance: 2647.6  on 278  degrees of freedom
## AIC: 5555.5
## 
## Number of Fisher Scoring iterations: 1
## 
## 
##               Theta:  8.36 
##           Std. Err.:  3.29 
## 
##  2 x log-likelihood:  -5549.525</code></pre>
<p>This generates incredibly small standard errors and makes me skeptical… Because the results are now so much better than our unadjusted model, I really don’t trust it.</p>
</div>
<div id="zero-inflated-poisson" class="section level4">
<h4>Zero-Inflated Poisson</h4>
<p>Sometimes you get count data with loads of zeros. For example, a former student of mine was looking at how social life (hours per week spent socializing) and stress (credit hours + work hours per week) predicted number of alcoholic beverages consumed each week. But many students do not consume any alcohol at all!</p>
<pre class="r"><code>drinkdat&lt;-read.csv(&quot;~/Downloads/drink_data_total.csv&quot;)
drinkdat&lt;-drinkdat%&gt;%filter(Drinks&lt;20)
drinkdat%&gt;%ggplot(aes(Drinks))+geom_bar()</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>What are we to do? Well, we get creative! Think of trying to emulate the data-generating process: there are two overlapping populations here, drinkers and non-drinkers. Among drinkers, we may see a poisson distribution for drinks consumed in a week, but not among non-drinkers. However, non-drinkers are contributing to the zeros in our data.</p>
<pre class="r"><code>poisfit&lt;-glm(Drinks~Social*Stress,data=drinkdat,family=&quot;poisson&quot;)
summary(poisfit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Drinks ~ Social * Stress, family = &quot;poisson&quot;, data = drinkdat)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.4563  -1.8425  -1.5876   0.5853   5.3410  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -1.740315   0.548228  -3.174 0.001501 ** 
## Social         0.447490   0.081164   5.513 3.52e-08 ***
## Stress         0.062919   0.020656   3.046 0.002319 ** 
## Social:Stress -0.013059   0.003422  -3.816 0.000136 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 485.99  on 102  degrees of freedom
## Residual deviance: 440.71  on  99  degrees of freedom
## AIC: 586.33
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<pre class="r"><code>pchisq(440.71,99,lower.tail = F)</code></pre>
<pre><code>## [1] 1.293779e-44</code></pre>
<pre class="r"><code>exp&lt;-dpois(0:14,mean(drinkdat$Drinks))*length(drinkdat$Drinks)
obs&lt;-table(factor(drinkdat$Drinks,levels=0:14))
chisq&lt;-sum((obs-exp)^2/exp)
pchisq(chisq,df = 13,lower.tail = F)</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>library(pscl)</code></pre>
<pre><code>## Classes and Methods for R developed in the
## Political Science Computational Laboratory
## Department of Political Science
## Stanford University
## Simon Jackman
## hurdle and zeroinfl functions by Achim Zeileis</code></pre>
<pre><code>## 
## Attaching package: &#39;pscl&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     prussian</code></pre>
<pre class="r"><code>zip_fit&lt;-zeroinfl(Drinks~Social*Stress|1,data=drinkdat)
summary(zip_fit)</code></pre>
<pre><code>## 
## Call:
## zeroinfl(formula = Drinks ~ Social * Stress | 1, data = drinkdat)
## 
## Pearson residuals:
##     Min      1Q  Median      3Q     Max 
## -0.7407 -0.7081 -0.6985  0.3800  4.1894 
## 
## Count model coefficients (poisson with log link):
##                Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)    0.426825   0.479516   0.890  0.37340   
## Social         0.209253   0.072870   2.872  0.00408 **
## Stress         0.044174   0.015595   2.833  0.00462 **
## Social:Stress -0.007978   0.002728  -2.925  0.00345 **
## 
## Zero-inflation model coefficients (binomial with logit link):
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   0.4004     0.2024   1.978    0.048 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 
## 
## Number of iterations in BFGS optimization: 18 
## Log-likelihood: -174.8 on 5 Df</code></pre>
<p>This model fits much better than the non-zero-inflated poisson,</p>
<pre class="r"><code>vuong(zip_fit,poisfit)</code></pre>
<pre><code>## Vuong Non-Nested Hypothesis Test-Statistic: 
## (test-statistic is asymptotically distributed N(0,1) under the
##  null that the models are indistinguishible)
## -------------------------------------------------------------
##               Vuong z-statistic             H_A    p-value
## Raw                    5.891903 model1 &gt; model2 1.9089e-09
## AIC-corrected          5.840380 model1 &gt; model2 2.6041e-09
## BIC-corrected          5.772504 model1 &gt; model2 3.9051e-09</code></pre>
<p>Here the test statistic is significant across the board indicating that the zero-inflated Poisson model fits better than the standard Poisson model.</p>
</div>
<div id="zero-truncated-poisson" class="section level4">
<h4>Zero-Truncated Poisson</h4>
<p>Sometimes, instead of an abundance of true zeros, you have strictly non-zero data (e.g., count data where the minimum value is 1). An ordinary Poisson regression will try to predict zero counts, which would be inappropriate for this data.</p>
<p>What if you were interested in the affect of socializing and stress on number of drinks consumed <em>among students who drink</em>.</p>
<pre class="r"><code>drinkdat_nozero&lt;-drinkdat%&gt;%filter(Drinks&gt;0)

#install.packages(&quot;VGAM&quot;)
library(VGAM)</code></pre>
<pre><code>## Loading required package: stats4</code></pre>
<pre><code>## Loading required package: splines</code></pre>
<pre><code>## 
## Attaching package: &#39;VGAM&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     fill</code></pre>
<pre class="r"><code>trunc_fit&lt;-vglm(Drinks~Social*Stress,family=pospoisson(),data=drinkdat_nozero)
summary(trunc_fit)</code></pre>
<pre><code>## 
## Call:
## vglm(formula = Drinks ~ Social * Stress, family = pospoisson(), 
##     data = drinkdat_nozero)
## 
## Pearson residuals:
##                   Min     1Q  Median    3Q   Max
## loglink(lambda) -1.85 -1.172 -0.2817 1.054 4.026
## 
## Coefficients: 
##                Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)    0.435122   0.477608   0.911  0.36227   
## Social         0.209062   0.073625   2.840  0.00452 **
## Stress         0.044434   0.015720   2.827  0.00470 **
## Social:Stress -0.008062   0.002818  -2.861  0.00423 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Name of linear predictor: loglink(lambda) 
## 
## Log-likelihood: -105.574 on 37 degrees of freedom
## 
## Number of Fisher scoring iterations: 4 
## 
## No Hauck-Donner effect found in any of the estimates</code></pre>
<pre class="r"><code>#AIC
-2*-105.574+2*3</code></pre>
<pre><code>## [1] 217.148</code></pre>
<pre class="r"><code>summary(glm(Drinks~Social*Stress,family=poisson,data=drinkdat_nozero))</code></pre>
<pre><code>## 
## Call:
## glm(formula = Drinks ~ Social * Stress, family = poisson, data = drinkdat_nozero)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2303  -1.2847  -0.2845   0.9985   3.2402  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)    0.471948   0.469777   1.005  0.31508   
## Social         0.203073   0.072302   2.809  0.00497 **
## Stress         0.043329   0.015533   2.789  0.00528 **
## Social:Stress -0.007817   0.002751  -2.842  0.00449 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 82.526  on 40  degrees of freedom
## Residual deviance: 74.156  on 37  degrees of freedom
## AIC: 219.78
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Slightly lower AIC, slightly better fit compared to using a poisson regression. Results are extremely comparable.</p>
</div>
</div>
</div>
