---
title: 'Basic Stats U Need #2: T-Test'
author: Nathaniel Raley Woodward
date: '2017-08-23'
slug: basic-stats-u-need-t-test
categories: []
tags:
  - R Markdown
  - Introductory Stats
  - Sampling Distribution
  - T-Test
  - Student's T Distribution
---



<div id="part-2-the-t-distribution" class="section level2">
<h2>Part 2: the t-Distribution</h2>
<p>We saw in the previous post that if X is a random variable and the population distribution of X is normal with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> (variance <span class="math inline">\(\sigma^2\)</span>) then, the distribution of the sample mean <span class="math inline">\(\bar{X}\)</span> for samples of a given size <span class="math inline">\(n\)</span> is normal, with mean <span class="math inline">\(\mu\)</span> and standard deviation of <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>, which we can write <span class="math inline">\(\bar{X}_n \sim N(\mu,\frac{\sigma}{\sqrt{n}})\)</span>.[^1] More exciting, we saw that by the Central Limit Theorem, the sampling distribution will be normal <em>regardless of the original population distribution</em> if the sample size is large enough.</p>
<p>Recall that a Z-score is computed <span class="math inline">\(Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\)</span> If you haven’t seen this before, realize that we are just rescaling our sampling distribution from the previous post, <span class="math inline">\(\bar{X} \sim N(\mu,\frac{\sigma}{\sqrt{n}})\)</span>, in order to preserve all of the information while setting the mean to 0 and the standard deviation to 1. Another way to think of it is than, instead of dealing in terms of the sample mean <span class="math inline">\(\bar{X}\)</span>, we want to deal in terms of the distances of the sample mean from the population mean <span class="math inline">\(\bar{X}-\mu\)</span> in units of standard deviation <span class="math inline">\(\sigma\)</span>, and we deal with that by transforming <span class="math inline">\(\bar{X} \sim N(\mu,\sigma^2/n)\)</span> into <span class="math inline">\(Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\)</span>.</p>
<p>We can do this because a normal random variable <span class="math inline">\(X\)</span> is still normal when it undergoes a linear transformation (multiplying it, adding to it, etc). This means that if <span class="math inline">\(X\sim N(\mu,\sigma)\)</span>, then <span class="math inline">\(Y=aX+b\)</span> is also normal, <span class="math inline">\(Y\sim N(a\mu+b,a\sigma)\)</span>. Using this property,</p>
<p><span class="math display">\[\bar{X}-\mu \sim N(\mu-\mu,\frac{\sigma}{\sqrt{n}}) = N(0,\frac{\sigma}{\sqrt{n}})\]</span> And</p>
<p><span class="math display">\[\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(\frac{0}{\sigma/\sqrt{n}},\frac{\sigma/\sqrt{n}}{\sigma/\sqrt{n}})= N(0,1)\]</span>.</p>
<p>Here’s a quick example just so you can see this in action: the heights (in inches) of adult women in the US are distributed as <span class="math inline">\(N(63.8,2.7)\)</span>. Let’s say you, <span class="math inline">\(X_1\)</span>, are a 5’8&quot; woman (68 inches tall).</p>
<pre class="r"><code>dist&lt;-rnorm(10000,63.8,2.7)

{hist(dist,breaks=100,main=&quot;&quot;,prob=T)
curve(dnorm(x,63.8,2.7),add=T,col=&quot;blue&quot;)
abline(v=68,col=&quot;red&quot;)
text(68,.1,&quot;68&quot;)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code># Now we subtract the population mean from every observation in our sample

dist.minus.mean&lt;-dist-63.8
{hist(dist.minus.mean,breaks=100,main=&quot;&quot;,prob=T)
  curve(dnorm(x+63.8,63.8,2.7),add=T,col=&quot;blue&quot;)
#notice how adding the mean y=f(x+63.8)
abline(v=68-63.8,col=&quot;red&quot;)
text(4.2,.1,&quot;4.2&quot;)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code># See how nothing changed except the x-axis; we have effectively scooted the entire distribution right, so that it is centered at zero. Now let&#39;s divide each observation by the population standard deviation$

dist.minus.mean.dividedby.sd&lt;-dist.minus.mean/2.7
{hist(dist.minus.mean.dividedby.sd,breaks=100,main=&quot;&quot;,prob=T)
curve(dnorm(x,0,1),add=T,col=&quot;blue&quot;)
abline(v=(68-63.8)/2.7, col=&quot;red&quot;)
text(2,.3,labels=&quot;1.555555&quot;)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
<p>I drew a sample of 10,000 from such a distribution just to illustrate how subtracting the mean from <em>each</em> of those 10,000 values and then dividing <em>each</em> by the standard deviation preserves the normality of the sample (see distribution overlayed in blue).</p>
<p>Recall too that, if our statistic is normally distributed, 95% of the density should lie within 1.96 standard deviations of the mean. Here, <code>pnorm</code> is the CDF for the normal distribution: it gives us the area under the curve from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(q\)</span>, where q is a Z-score.</p>
<pre class="r"><code>pnorm(1.96)-pnorm(-1.96)</code></pre>
<pre><code>## [1] 0.9500042</code></pre>
<p>OK, now back to STUDENT:</p>
<center>
<img src="student1.png" />
</center>
<p><br></p>
<p>Here’s what he means. Let’s take repeated samples of 10 students’ heights and calculate the <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(s\)</span> for each.</p>
<pre class="r"><code>samps10&lt;-matrix(nrow=10,ncol=10000)
samps10&lt;-replicate(10000,sample(pop.height,10))
samps.mean&lt;-apply(samps10,2,mean)
samps.sd&lt;-apply(samps10,2,sd)</code></pre>
<p>Now, is it really true that, when we substitute <span class="math inline">\(s\)</span> for <span class="math inline">\(\sigma\)</span> that 95% of the time the true population mean lies within <span class="math inline">\(\bar{X}\pm 1.96*s/\sqrt{n}\)</span>?</p>
<pre class="r"><code>s10.s&lt;-mean(samps.mean-1.96*samps.sd/sqrt(10)&gt; pop.mean | pop.mean&gt;samps.mean+1.96*samps.sd/sqrt(10))
s10.s</code></pre>
<pre><code>## [1] 0.0828</code></pre>
<p>Hmm, it looks like the population mean is actually outside this range 8.28% of the time. Clearly, when we do not know <span class="math inline">\(\sigma\)</span>, we cannot substitute <span class="math inline">\(s\)</span> with impunity. Again, had we known <span class="math inline">\(s\)</span>, things would have been fine:</p>
<pre class="r"><code>samps10&lt;-matrix(nrow=10,ncol=10000)
samps10&lt;-replicate(10000,sample(pop.height,10))
samps.mean&lt;-apply(samps10,2,mean)
s10.sigma&lt;-mean(samps.mean-1.96*pop.sd/sqrt(10)&gt; pop.mean | pop.mean&gt;samps.mean+1.96*pop.sd/sqrt(10))
s10.sigma</code></pre>
<pre><code>## [1] 0.048</code></pre>
<p>Using the population standard deviation, we find that the population mean falls outside two standard errors of the sample mean just 4.8% of the time. But this is no help to us in the real world!</p>
<p>Things just get worse with smaller samples. Here’s what happens if we just have <span class="math inline">\(n=3\)</span></p>
<pre class="r"><code>samps3&lt;-matrix(nrow=3,ncol=10000)
samps3&lt;-replicate(10000,sample(pop.height,3))
samps3.mean&lt;-apply(samps3,2,mean)
samps3.sd&lt;-apply(samps3,2,sd)
s3.s&lt;-mean(samps3.mean-1.96*samps3.sd/sqrt(3)&gt; pop.mean | pop.mean&gt;samps3.mean+1.96*samps3.sd/sqrt(3))
s3.s</code></pre>
<pre><code>## [1] 0.2073</code></pre>
<p>Yikes, now the population mean falls outside our 95% confidence interval 20.73% of the time; clearly, assuming normality is inappropriate when samples are small and we are using <span class="math inline">\(s\)</span> instead of <span class="math inline">\(\sigma\)</span>.</p>
<p>This problem was solved by William Sealy Gosset (“Student”“) in 1908, whose paper is excerpted throughout this post</p>
<center>
<img src="student3.png" />
</center>
<p><br></p>
<p>The “alternative” he furnishes is none other than the <span class="math inline">\(t\)</span> distribution.[^2] I will walk us through the derivations in his celebrated paper at the bottom, but as makes for an enormous, excruciating tangent, I will give a brief overview:</p>
<ol style="list-style-type: decimal">
<li><p>First, he determines the sampling distribution of standard deviations drawn from a normal population; he finds that this agrees with the Chi-squared distribution</p></li>
<li><p>The, he shows that there is no correlation between the sample mean and the sample standard deviation (suggesting that the two random variables are independent).</p></li>
<li><p>Finally, he determines the distribution of t (which he calls <span class="math inline">\(z\)</span>), which is the distance between the sample mean and the population mean, divided by the sample standard deviation <span class="math inline">\(\frac{\bar{x}-\mu}{s/\sqrt{n}}\)</span> (note the <em>n</em> in the denominator instead of our <em>n-1</em>)</p></li>
</ol>
<p>First, he shows that</p>
<p><span class="math display">\[ 
x=\frac{(n-1)}{\sigma^2}s^2 \sim \chi^2_{n-1}, \text{ that is, it has the density}\\
p(x|n) =\frac{1}{2^{\frac{n-1}{2}}\Gamma(\frac{n-1}{2})}x^{\frac{n-1}{2}-1}e^{-\frac{x}{2}}
\]</span></p>
<p>Can we confirm this? Recall that our population variance <span class="math inline">\(\sigma^2\)</span> was 15.5544059, so let’s plot ’em and see if this density fits</p>
<pre class="r"><code>#The distribution of sample variance when n=3
n=3
{hist((n-1)/(pop.sd^2)*samps3.sd^2,prob=T,breaks=50)
curve(1/(2^((n-1)/2)*gamma((n-1)/2))*x^((n-1)/2-1)*exp(-x/2),xlim=c(0,30),add=T)
curve(dchisq(x,df=2),add=T)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>#The distribution of the sample variance when n=10
n=10
{hist((n-1)/(pop.sd^2)*samps.sd^2,prob=T,breaks=50)
curve(1/(2^((n-1)/2)*gamma((n-1)/2))*x^((n-1)/2-1)*exp(-x/2),xlim=c(0,30),add=T)
curve(dchisq(x,df=9),add=T)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<p>Looks pretty good! Finally, he finds the distribution of the distances of the sample mean to the true mean <span class="math inline">\(\bar{X_n}-\mu\)</span>, divided by the standard deviation of the sample mean <span class="math inline">\(s/\sqrt{n}\)</span> (instead of dividing by <span class="math inline">\(\sigma/\sqrt{n}\)</span>; see previous post).</p>
<p><span class="math display">\[
t=\frac{\bar{X}-\mu}{s/\sqrt{n}}, \text{ which has the density}\\
p(t|n)=\frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n-1}{2})} \frac{1}{\sqrt{(n-1)\pi}}\left(1+\frac{t^2}{n-1}\right)^{-n/2}
\]</span></p>
<p>Replacing <span class="math inline">\(\sigma\)</span> with <span class="math inline">\(s\)</span> in our Z-score formula gives a statistic that follows …you guessed it, the <em>t</em> distribution! The function itself looks way different that the normal density function <span class="math inline">\(p(x|\mu,\sigma)=\frac{1}{\sqrt{2 \pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span>. First of all, notice that it doesn’t depend on <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\sigma\)</span> at all; let’s see how it actually looks compared to a normal distirbution</p>
<pre class="r"><code>t.pdf&lt;-function(x,n){gamma(n/2)/(sqrt((n-1)*pi)*gamma((n-1)/2))*(1+x^2/(n-1))^(-n/2)}
{curve(t.pdf(x,3),xlim=c(-4,4),ylim=c(0,.4),col=&quot;red&quot;)
  curve(t.pdf(x,4),xlim=c(-4,4),ylim=c(0,.4),col=&quot;red&quot;,add=T,lty=2)
  curve(t.pdf(x,6),xlim=c(-4,4),ylim=c(0,.4),col=&quot;red&quot;,add=T,lty=3)
  curve(t.pdf(x,11),xlim=c(-4,4),ylim=c(0,.4),col=&quot;red&quot;,add=T,lty=4)
  curve(t.pdf(x,26),xlim=c(-4,4),ylim=c(0,.4),col=&quot;red&quot;,add=T,lty=5)
curve(dnorm(x,0,1),add=T,col=&quot;blue&quot;)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Above, we have plotted five <em>t</em> distributions (red) with 2, 3, 5, 10, and 25 degrees of freedom. Notice that the distribution with df=25 is almost indistinguishable from the normal distribution (blue). In fact, though it wasn’t proven until much later, <span class="math inline">\(t_n \rightarrow N(0,1)\ as \ n \rightarrow \infty\)</span></p>
<p>Does this jibe with our observed data better than the normal? Let’s look at our samples of size 3 again and plot their respective t-statistics:</p>
<pre class="r"><code>n=3
ts=(samps3.mean-pop.mean)/(samps3.sd/sqrt(n))
{hist(ts,prob=T,breaks=500,xlim=c(-10,10))
curve(dt(x,df=2),add=T,col=&quot;red&quot;)
curve(dnorm(x,0,samps3.sd[1]/sqrt(n)),add=T,col=&quot;blue&quot;)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>n=10
ts=(samps.mean-pop.mean)/(samps.sd/sqrt(n))
{hist(ts,prob=T,breaks=50,xlim=c(-5,5))
curve(dt(x,df=9),add=T,col=&quot;red&quot;)
curve(dnorm(x,0,samps.sd[1]/sqrt(n)),add=T,col=&quot;blue&quot;)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<p>Looks much better! Remember how when we assumed that the means of samples of size 3 were normally distributed, our 95% confidence interval (the interval from Z=-1.96 to Z=1.96) only included the mean 79.27% of the time? In a <span class="math inline">\(t\)</span> distribution with n-1 degrees of freedom, 95% of the distribution lies within 4.3 <em>sample</em> standard deviations of the population mean.</p>
<pre class="r"><code>qt(c(.025,.975),2)</code></pre>
<pre><code>## [1] -4.302653  4.302653</code></pre>
<pre class="r"><code>x1&lt;-seq(-6,-4.3,len=100)
x2&lt;-seq(4.3,6,len=100)
y1&lt;-dt(x1,2)
y2&lt;-dt(x2,2)
x3&lt;-seq(-6,-1.96,len=100)
x4&lt;-seq(1.96,6,len=100)
y3&lt;-dnorm(x3)
y4&lt;-dnorm(x4)
{curve(dt(x,2), from=-6, to=6,ylim=c(0,.4),col=&quot;red&quot;)
curve(dnorm(x),from=-6, to=6,add=T,col=&quot;blue&quot;)
polygon(c(x1[1],x1,x1[100]),c(0,y1,0),col=&quot;red&quot;,border=NA)
polygon(c(x2[1],x2,x2[100]),c(0,y2,0),col=&quot;red&quot;,border=NA)
polygon(c(x3[1],x3,x3[100]),c(0,y3,0),col=&quot;blue&quot;,border=NA)
polygon(c(x4[1],x4,x4[100]),c(0,y4,0),col=&quot;blue&quot;,border=NA)}</code></pre>
<p><img src="/post/2017-08-23-basic-stats-u-need-t-test_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The plot above shows a normal <span class="math inline">\(N(0,1)\)</span> distribution in blue and a <span class="math inline">\(t\)</span> distribution with 2 degrees of freedom in red; note that the red areas under the <span class="math inline">\(t\)</span> add up to 5% and the blue areas under the <span class="math inline">\(N(0,1)\)</span> add up to 5%.</p>
<p>Since the <span class="math inline">\(t\)</span>-statistic <span class="math inline">\(t=\frac{\bar{X}-\mu}{s/\sqrt{n}}\)</span> follows this distribution, we can rearrange it to find the confidence interval around the population mean.</p>
<p><span class="math display">\[
-t \le \frac{\bar{X}-\mu}{s/\sqrt{n}} \le t \\
\frac{-ts}{\sqrt{n}} \le \bar{X}-\mu \le \frac{ts}{\sqrt{n}} \\
\frac{-ts}{\sqrt{n}}-\bar{X} \le -\mu \le \frac{ts}{\sqrt{n}}-\bar{X} \\
\frac{ts}{\sqrt{n}}+\bar{X} \ge \mu \ge \frac{-ts}{\sqrt{n}}+\bar{X} \\
\bar{X}-\frac{ts}{\sqrt{n}} \le \mu \le \bar{X}+\frac{ts}{\sqrt{n}} \\
\]</span></p>
</div>
